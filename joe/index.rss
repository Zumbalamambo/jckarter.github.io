<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel>
<title>Durian Software: Joe's Blog</title>
<link>http://duriansoftware.com/joe/</link>
<description>Joe's blog.</description>
<language>en-us</language>
<item>
<title>Self-aware struct-like types in C++11</title>
<link>http://duriansoftware.com/joe/Self-aware-struct-like-types-in-C++11.html</link>
<description>
&lt;p&gt;
Even with C++11, C++ offers inadequate metaprogramming facilities for user-defined types compared
to other programming languages.  Given an arbitrary struct type, you can't iterate its fields and
get useful information like name, offset, and size, without implementing those facilities by hand.
(Nearly every would-be C++ replacement language fixes this, but they
unfortunately aren't always viable options.)
The C++11 standard library introduced the &lt;tt&gt;tuple&lt;/tt&gt; template as a general-purpose
metaprogrammable composite type, but it sucks in a number of ways:
&lt;ul&gt;
&lt;li&gt;The user interface is awkward. Fields are unnamed and require integer indexing using a top-level template function, &lt;tt&gt;get&amp;lt;N&amp;gt;(tuple)&lt;/tt&gt; instead of &lt;tt&gt;struct.fieldname&lt;/tt&gt;.
&lt;li&gt;The implementation is not straightforward: &lt;a href="https://github.com/llvm-mirror/libcxx/blob/master/include/tuple"&gt;libc++'s &lt;tt&gt;tuple&lt;/tt&gt; header&lt;/a&gt; is 1054 lines, and &lt;a href="http://gcc.gnu.org/viewcvs/trunk/libstdc%2B%2B-v3/include/std/tuple?revision=188636&amp;view=markup"&gt;libstdc++'s&lt;/a&gt; is 1100, and both implementations rely on several layers of internal base classes to form the user-facing &lt;tt&gt;tuple&lt;/tt&gt; type.
&lt;li&gt;Useful techniques like &lt;a href="http://stackoverflow.com/questions/7858817/unpacking-a-tuple-to-call-a-matching-function-pointer"&gt;passing unpacked tuple elements to a function&lt;/a&gt; are awkward to implement.
&lt;li&gt;&lt;tt&gt;tuple&lt;/tt&gt; as specified by the standard doesn't provide &lt;tt&gt;constexpr&lt;/tt&gt; elementwise construction or field access and can't be used in compile-time calculations.
&lt;/ul&gt;
&lt;p&gt;Here's an alternative approach I came up with that provides a user interface nearly equivalent to primitive structs, is much easier to metaprogram with than &lt;tt&gt;tuple&lt;/tt&gt;, and is easier to implement as well, requiring about 150 lines of header-only code.
I've put a sample implementation up on Github at &lt;a href="https://github.com/jckarter/selfaware"&gt;https://&lt;wbr&gt;github.com/&lt;wbr&gt;jckarter/&lt;wbr&gt;selfaware&lt;/a&gt;. Here's a rundown of how it works.
&lt;h3&gt;Self-aware field templates&lt;/h3&gt;
&lt;p&gt;The main idea is to inherit the "struct" type from a set of field class templates, each of which defines a single field along with methods to generically access its name string, value, and type. For example, a field template named &lt;tt&gt;foo&lt;/tt&gt; looks like this:
&lt;pre&gt;template&amp;lt;typename T&gt;
struct foo {
    T foo;

    // field name
    constexpr static char const *name() { return "foo"; }

    // field type
    using type = T;

    // field value generic accessor
    T &amp;amp;value() &amp;amp; { return this-&gt;foo; }
    T const &amp;amp;value() const &amp;amp; { return this-&gt;foo; }
    T &amp;amp;&amp;amp;value() &amp;amp;&amp;amp; { return this-&gt;foo; }
};&lt;/pre&gt;
&lt;p&gt;A preprocessor macro can generate these for us:
&lt;pre&gt;#define SELFAWARE_IDENTIFIER(NAME) \
    template&amp;lt;typename T&gt; \
    struct NAME { \
        T NAME; \
        // field name \
        constexpr static char const *name() { return #NAME; } \
        // field type \
        using type = T; \
        // field value generic accessor \
        T &amp;amp;value() &amp;amp; { return this-&gt;NAME; } \
        T const &amp;amp;value() const &amp;amp; { return this-&gt;NAME; } \
        T &amp;amp;&amp;amp;value() &amp;amp;&amp;amp; { return this-&gt;NAME; } \
    };&lt;/pre&gt;
&lt;h3&gt;The self-aware struct template&lt;/h3&gt;
&lt;p&gt;The "struct" template now needs only to inherit a set of field template instances and provide some
constructors:
&lt;pre&gt;template&amp;lt;typename...Fields&gt;
struct Struct : Fields... {
    // A convenience alias for subclasses
    using struct_type = Struct;

    // Preserve default constructors
    Struct() = default;
    Struct(Struct const &amp;amp;) = default;

    // Forwarding elementwise constructor
    template&amp;lt;typename...T&gt;
    constexpr Struct(T &amp;amp;&amp;amp;...x) : Fields{static_cast&amp;lt;T&amp;amp;&amp;amp;&gt;(x)}... {}
};&lt;/pre&gt;
&lt;p&gt;A &lt;tt&gt;Struct&lt;/tt&gt; type can then be used either by aliasing a &lt;tt&gt;Struct&lt;/tt&gt; instance or by
inheriting an instance and its constructors.
(As of Clang 3.1 and GCC 4.7, neither compiler yet supports inheriting constructors, so
aliasing is currently more practical.)
&lt;pre&gt;SELFAWARE_IDENTIFIER(foo)
SELFAWARE_IDENTIFIER(bar)
// Aliasing a Struct instance
using FooBar = Struct&amp;lt;foo&amp;lt;int&gt;, bar&amp;lt;double&gt;&gt;;
// Inheriting a Struct instance (requires inheriting constructors)
struct FooBar2 : Struct&amp;lt;foo&amp;lt;int&gt;, bar&amp;lt;double&gt;&gt; { using struct_type::struct_type; };&lt;/pre&gt;
&lt;p&gt;Values of the type look like normal structs to user code:
&lt;pre&gt;FooBar frob(int x) {
    FooBar f = {x, 0.0};
    f.foo += 1;
    f.bar += 1.0;
    return f;
}&lt;/pre&gt;
&lt;p&gt;The type is &lt;a href="http://en.cppreference.com/w/cpp/types/is_trivial"&gt;trivial&lt;/a&gt; if its component types are trivial, and its instances can be used in compile-time calculations if its component types can, like primitive structs. (However, because it inherits
multiple nonempty types, it's not &lt;a href="http://en.cppreference.com/w/cpp/types/is_standard_layout"&gt;standard-layout&lt;/a&gt;, and thus not quite POD.)
&lt;pre&gt;static_assert(std::is_trivial&amp;lt;FooBar&gt;::value, "should be trivial");
static_assert(FooBar{2, 3.0}.foo + FooBar{2, 4.0}.foo == 4, "2 + 2 == 4");&lt;/pre&gt;
&lt;h3&gt;Metaprogramming with self-aware structs&lt;/h3&gt;
&lt;p&gt;Since the fields of the &lt;tt&gt;Struct&lt;/tt&gt; template are encoded in a template parameter pack,
there's a lot you can do to it with unpack expressions and recursive templates. Here are a few
examples:

&lt;h3&gt;Function application&lt;/h3&gt;
&lt;p&gt;Applying a function object
to a &lt;tt&gt;Struct&lt;/tt&gt;'s unpacked fields is easy&amp;mdash;just unpack the &lt;tt&gt;value()&lt;/tt&gt; method
of each field superclass into a function call expression:
&lt;pre&gt;template&amp;lt;typename Function, typename...Fields&gt;
auto apply(Function &amp;amp;&amp;amp;f, Struct&amp;lt;Fields...&gt; const &amp;amp;a_struct)
    -&gt; decltype(f(a_struct.Fields::value()...))
{
    return f(a_struct.Fields::value()...);
}

double hypotenuse(double x, double y) { return sqrt(x*x, y*y); }

double fooBarHypotenuse(FooBar const &amp;amp;x) { return apply(hypotenuse, x); }&lt;/pre&gt;

&lt;h3&gt;Interop with &lt;tt&gt;tuple&lt;/tt&gt;&lt;/h3&gt;

&lt;p&gt;A &lt;tt&gt;Struct&lt;/tt&gt; can be converted into a &lt;tt&gt;tuple&lt;/tt&gt; or &lt;tt&gt;tie&lt;/tt&gt; similarly:
&lt;pre&gt;template&amp;lt;typename...Fields&gt;
auto structToTuple(Struct&amp;lt;Fields...&gt; const &amp;amp;s)
    -&gt; std::tuple&amp;lt;typename Fields::type...&gt;
{
    return std::make_tuple(s.Fields::value()...);
}
template&amp;lt;typename...Fields&gt;
void assignStructFromTuple(Struct&amp;lt;Fields...&gt; &amp;amp;s,
                           std::tuple&amp;lt;typename Fields::type...&gt; const &amp;amp;t)
{
    std::tie(s.Fields::value()...) = t;
}&lt;/pre&gt;

&lt;h3&gt;Generating code from field metadata&lt;/h3&gt;
&lt;p&gt;The &lt;tt&gt;Struct&lt;/tt&gt; template can implement a static method to iterate through its fields,
passing the name string, offset, size, and type of each field to a function object in turn.
(Getting the offset unfortunately relies on undefined behavior, because C++11 restricts
&lt;tt&gt;offsetof&lt;/tt&gt; to standard-layout types and provides no other well-defined means that
I know of for determining offsets independent of an instance.)

&lt;pre&gt;template&amp;lt;typename...Fields&gt;
struct Struct : Fields... {
    // ... see above ...

    // NB: relies on undefined behavior
    template&amp;lt;typename Field&gt;
    static std::uintptr_t offset_of() {
        return reinterpret_cast&amp;lt;std::uintptr_t&gt;(&amp;amp;static_cast&amp;lt;Struct*&gt;(nullptr)-&gt;Field::value());
    }

    template&amp;lt;template&amp;lt;typename T&gt; class Trait, typename Function&gt;
    static void each_field(Function &amp;amp;&amp;amp;f)
    {
        // Unpack expressions are only allowed in argument lists and initialization lists,
        // so this expression unpacks the function call expression into the initializer list
        // for an unused array (which the optimizer is nice enough to discard)
        char pass[] = {
            (f(Fields::name(), offset_of&amp;lt;Fields&gt;(), sizeof(typename Fields::type),
              Trait&amp;lt;typename Fields::type&gt;::value()), '\0')...};
        (void)pass; // suppress unused variable warnings
    }
};&lt;/pre&gt;

&lt;p&gt;Many libraries that deal with binary data have finicky APIs for describing struct layouts.
A good example is OpenGL's &lt;a href="http://www.opengl.org/sdk/docs/man4/"&gt;&lt;tt&gt;gl&lt;wbr&gt;Vertex&lt;wbr&gt;Attrib&lt;wbr&gt;Pointer&lt;/tt&gt;&lt;/a&gt; interface, which is used to describe
the format of vertex information in memory. The &lt;tt&gt;each_field&lt;/tt&gt;
function template, paired with a traits class, can generate the correct sequence of
&lt;tt&gt;gl&lt;wbr&gt;Vertex&lt;wbr&gt;Attrib&lt;wbr&gt;Pointer&lt;/tt&gt; automatically from a &lt;tt&gt;Struct&lt;/tt&gt; instance's metadata:

&lt;pre&gt;struct GLVertexType { GLuint size; GLenum type; GLboolean normalized; };

// A trait class to provide glVertexAttribPointer arguments appropriate for a type
template&amp;lt;typename&gt; struct GLVertexTraits;
template&amp;lt;GLuint N&gt;
struct GLVertexTraits&amp;lt;float[N]&gt; {
    static GLVertexType value() { return {N, GL_FLOAT, GL_FALSE}; }
};
template&amp;lt;GLuint N&gt;
struct GLVertexTraits&amp;lt;std::uint8_t[N]&gt; {
    static GLVertexType value() { return {N, GL_UNSIGNED_BYTE, GL_TRUE}; }
};

template&amp;lt;typename Struct&gt;
bool bindVertexAttributes(GLuint program)
{
    _VertexAttributeBinder iter(program, sizeof(T));
    Struct::template each_field&amp;lt;GLVertexTraits&gt;(
        [=program](char const *name, size_t offset, size_t size, GLVertexType info) {
            GLint location = glGetAttribLocation(program, name);
            glVertexAttribPointer(location, info.size, info.type, info.normalized,
                                  sizeof(Struct), reinterpret_cast&amp;lt;const GLvoid*&gt;(offset));
            glEnableVertexAttribArray(location);
        });
    return iter.ok;
}&lt;/pre&gt;

&lt;h3&gt;Selecting a field at runtime by string name&lt;/h3&gt;
&lt;p&gt;A recursive template can generate code to pick a field at runtime from a string argument, passing the value through a function object to narrow the return type:&lt;/p&gt;
&lt;pre&gt;template&amp;lt;typename R, typename Field, typename...Fields, typename Visitor, typename...AllFields&gt;
R _select_field(Visitor &amp;amp;&amp;amp;v, char const *name, Struct&amp;lt;AllFields...&gt; const &amp;amp;a_struct)
{
    if (strcmp(name, Field::name()) == 0)
        return v(a_struct.Field::value());
    else
        return _select_field&amp;lt;R, Fields...&gt;(static_cast&amp;lt;Visitor&amp;amp;&amp;amp;&gt;(v), name, a_struct);
}

template&amp;lt;typename R, typename Visitor, typename...AllFields&gt;
R _select_field(Visitor &amp;amp;&amp;amp;v, char const *name, Struct&amp;lt;AllFields...&gt; const &amp;amp;a_struct)
{
    throw std::runtime_error("bad field name");
}

template&amp;lt;typename Visitor, typename Field, typename...AllFields&gt;
auto select_field(Visitor &amp;amp;&amp;amp;v, char const *name, Struct&amp;lt;Field, AllFields...&gt; const &amp;amp;a_struct)
    -&gt; decltype(v(a_struct.Field::value()))
{
    return _select_field&amp;lt;decltype(v(a_struct.Field::value())), Field, AllFields...&gt;
        (static_cast&amp;lt;Visitor&amp;amp;&amp;amp;&gt;(v), name, a_struct);
}&lt;/pre&gt;

&lt;tt&gt;select_field&lt;/tt&gt; can then be used like this:

&lt;pre&gt;template&amp;lt;typename T&gt;
struct converter {
    template&amp;lt;typename U&gt; T operator()(U &amp;amp;&amp;amp;x) { return T(x); }
};

void testStructSelectField()
{
    FooBar x{11, 22.0};

    double foo = select_field(converter&amp;lt;double&gt;(), "foo", x);
    double bar = select_field(converter&amp;lt;double&gt;(), "bar", x);
    assert(foo == 11.0);
    assert(bar == 22.0);
}&lt;/pre&gt;

&lt;h3&gt;Problems&lt;/h3&gt;
&lt;p&gt;This technique still isn't ideal. Most obviously, field templates all need to be
defined somewhere, which adds maintenance friction, and they rely on unseemly preprocessor magic
to create. Fields and &lt;tt&gt;Struct&lt;/tt&gt; instances could
perhaps be instantiated together in one macro, perhaps by pulling in &lt;tt&gt;boost::preprocessor&lt;/tt&gt;.
Compile time, always an issue with C++, also suffers from use of the &lt;tt&gt;Struct&lt;/tt&gt; template. Clang 3.1 takes almost a second on this 2.4 GHz Core 2 Duo just to compile the 199-line &lt;a href="https://github.com/jckarter/selfaware/blob/master/selfaware-test.cpp"&gt;&lt;tt&gt;selfaware-test.cpp&lt;/tt&gt;&lt;/a&gt; test suite. And &lt;tt&gt;tuple&lt;/tt&gt;, for all its faults, is standard, and will be available on any platform
that purports to support C++11. Neither &lt;tt&gt;Struct&lt;/tt&gt; nor &lt;tt&gt;tuple&lt;/tt&gt; is standard-layout and
thus can't interoperate with C in a standard-guaranteed, portable way. I'd love to hear about other
approaches to enabling composite type metaprogramming in C++.


</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2012-07-05T18:49:30+00:00</dc:date>
<guid>http://duriansoftware.com/joe/Self-aware-struct-like-types-in-C++11.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 4: Rendering a Dynamic 3D Scene with Phong Shading</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html"&gt;&amp;laquo; Chapter 3&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;At this point, we've seen the most important core parts of the OpenGL API and gotten a decent taste of the GLSL language. Now's a good time to start exercising OpenGL and implementing some graphic effects, introducing new nuances and specialized features of OpenGL and GLSL as we go. For the next few chapters, I've prepared a new demo program you can get from my Github &lt;a href="http://github.com/jckarter/ch4-flag"&gt;&lt;tt&gt;ch4-flag&lt;/tt&gt; repository&lt;/a&gt;. The &lt;tt&gt;flag&lt;/tt&gt; demo renders a waving flag on a flagpole against a simple background:
&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl4-flag.png"&gt;&lt;/center&gt;
&lt;p&gt;
With the flat, wallpaper-looking grass and brick textures and the unnatural lack of shadow cast by the flag, it looks like something a Nintendo 64 would have rendered, but it's a start. We'll improve the graphical fidelity of the demo over the next few chapters. For this chapter, we'll render the above image by implementing the &lt;a href="http://en.wikipedia.org/wiki/Phong_shading"&gt;&lt;b&gt;Phong shading&lt;/b&gt;&lt;/a&gt; model, which will serve as the basis for more advanced effects we'll look at later on.
&lt;/p&gt;
&lt;h3&gt;Overview of the &lt;tt&gt;flag&lt;/tt&gt; program&lt;/h3&gt;
&lt;p&gt;
I've organized &lt;tt&gt;flag&lt;/tt&gt; into four C files and four headers. You've already seen a good amount of it in &lt;tt&gt;hello-gl&lt;/tt&gt;: the &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/file-util.c"&gt;&lt;tt&gt;file-util.c&lt;/tt&gt;&lt;/a&gt; and &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/file-util.h"&gt;&lt;tt&gt;file-util.h&lt;/tt&gt;&lt;/a&gt; files contain the &lt;tt&gt;read_tga&lt;/tt&gt; and &lt;tt&gt;file_contents&lt;/tt&gt; functions, and &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/gl-util.c"&gt;&lt;tt&gt;gl-util.c&lt;/tt&gt;&lt;/a&gt; and &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/gl-util.h"&gt;&lt;tt&gt;gl-util.h&lt;/tt&gt;&lt;/a&gt; contain the &lt;tt&gt;make_texture&lt;/tt&gt;, &lt;tt&gt;make_shader&lt;/tt&gt;, and &lt;tt&gt;make_program&lt;/tt&gt; functions we wrote in chapter 2. The &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/gl-util.h"&gt;&lt;tt&gt;vec-util.h&lt;/tt&gt;&lt;/a&gt; header contains some basic vector math functions. &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/flag.c"&gt;&lt;tt&gt;flag.c&lt;/tt&gt;&lt;/a&gt; looks a lot like &lt;tt&gt;hello-gl.c&lt;/tt&gt; did: in &lt;tt&gt;main&lt;/tt&gt;, we initialize GLUT and GLEW, set up callbacks for GLUT events, call a &lt;tt&gt;make_resources&lt;/tt&gt; function to allocate a bunch of GL resources, and call out to &lt;tt&gt;glutMainLoop&lt;/tt&gt; to start running the demo. However, the setup and rendering are a bit more involved than last time. Let's look at what's new and changed:
&lt;/p&gt;
&lt;h3&gt;Mesh construction&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl4-mesh-01.png"&gt;
&lt;p&gt;
The &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/meshes.c"&gt;&lt;tt&gt;meshes.c&lt;/tt&gt;&lt;/a&gt; file contains code that generates the vertex and element arrays, collectively called a &lt;b&gt;mesh&lt;/b&gt;, for the flag, flagpole, ground, and wall objects that we'll be rendering. Most objects in the real world, including real flagpoles and flags, have smooth curving surfaces, but graphics cards deal with triangles. To render these objects, we have to approximate their surfaces as a collection of triangles. We do this by filling a vertex array with vertices placed along its surface, storing attributes of the surface with each vertex, and connecting the samples into triangles using the element array to give an approximation of the original surface.
&lt;/p&gt;
&lt;p&gt;The fundamental properties a mesh stores for each vertex are its &lt;b&gt;position&lt;/b&gt; in world space and its &lt;b&gt;normal&lt;/b&gt;, a vector perpendicular to the original surface. The normal is fundamental to shading calculations, as we'll see shortly. Normals should be &lt;b&gt;unit vectors&lt;/b&gt;, that is, vectors whose length is one. Each vertex also has &lt;b&gt;material&lt;/b&gt; parameters that indicate how the surface is shaded. The material can consist of a set of per-vertex values, &lt;b&gt;texture coordinates&lt;/b&gt; that sample material information from a texture, or some combination of both. &lt;p&gt;For the &lt;tt&gt;flag&lt;/tt&gt; demo, the material consists of a &lt;b&gt;texture coordinate&lt;/b&gt; for sampling the &lt;b&gt;diffuse&lt;/b&gt; color from the mesh texture, a &lt;b&gt;specular&lt;/b&gt; color, and &lt;b&gt;shininess&lt;/b&gt; factor. We'll see how these parameters are used shortly. Our vertex buffer thus contains an array of &lt;tt&gt;flag_vertex&lt;/tt&gt; structs looking like this:&lt;/p&gt;
&lt;pre&gt;struct flag_vertex {
    GLfloat position[4];
    GLfloat normal[4];
    GLfloat texcoord[2];
    GLfloat shininess;
    GLubyte specular[4];
};&lt;/pre&gt;
&lt;p&gt;Although the position and normal are three-dimensional vectors, we pad them out to four elements because most GPUs prefer to load vector data from 128-bit-aligned buffers, like SIMD instruction sets such as SSE. For each mesh, we collect the vertex buffer, element buffer, texture object, and element count into a &lt;tt&gt;flag_mesh&lt;/tt&gt; struct. When we render, we set up &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt;s to pass all of the &lt;tt&gt;flag_vertex&lt;/tt&gt; attributes to the vertex shader:&lt;/p&gt;
&lt;pre&gt; struct flag_mesh {
    GLuint vertex_buffer, element_buffer;
    GLsizei element_count;
    GLuint texture;
};&lt;/pre&gt;
&lt;pre&gt;static void render_mesh(struct flag_mesh const *mesh)
{
    glBindTexture(GL_TEXTURE_2D, mesh-&gt;texture);

    glBindBuffer(GL_ARRAY_BUFFER, mesh-&gt;vertex_buffer);
    glVertexAttribPointer(
        g_resources.flag_program.attributes.position,
        3, GL_FLOAT, GL_FALSE, sizeof(struct flag_vertex),
        (void*)offsetof(struct flag_vertex, position)
    );
    glVertexAttribPointer(
        g_resources.flag_program.attributes.normal,
        3, GL_FLOAT, GL_FALSE, sizeof(struct flag_vertex),
        (void*)offsetof(struct flag_vertex, normal)
    );
    glVertexAttribPointer(
        g_resources.flag_program.attributes.texcoord,
        2, GL_FLOAT, GL_FALSE, sizeof(struct flag_vertex),
        (void*)offsetof(struct flag_vertex, texcoord)
    );
    glVertexAttribPointer(
        g_resources.flag_program.attributes.shininess,
        1, GL_FLOAT, GL_FALSE, sizeof(struct flag_vertex),
        (void*)offsetof(struct flag_vertex, shininess)
    );
    glVertexAttribPointer(
        g_resources.flag_program.attributes.specular,
        4, GL_UNSIGNED_BYTE, GL_TRUE, sizeof(struct flag_vertex),
        (void*)offsetof(struct flag_vertex, specular)
    );

    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, mesh-&gt;element_buffer);
    glDrawElements(
        GL_TRIANGLES,
        mesh-&gt;element_count,
        GL_UNSIGNED_SHORT,
        (void*)0
    );
}&lt;/pre&gt;
&lt;p&gt;Note that the &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt; call for the &lt;tt&gt;specular&lt;/tt&gt; color attribute passes &lt;tt&gt;GL_TRUE&lt;/tt&gt; for the &lt;i&gt;normalized&lt;/i&gt; argument. The specular colors are stored as four-component arrays of bytes between &lt;tt&gt;0&lt;/tt&gt; and &lt;tt&gt;255&lt;/tt&gt;, much as they would be in a bitmap image, but with the normalized flag set, they'll be presented to the shaders as normalized floating-point values between &lt;tt&gt;0.0&lt;/tt&gt; and &lt;tt&gt;1.0&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The actual code to generate the meshes is fairly tedious, so I'll just describe it at a high level. We construct two distinct meshes: the background mesh, created by &lt;tt&gt;init_background_mesh&lt;/tt&gt;, which consists of the static flagpole, ground, and wall objects; and the flag, set up by &lt;tt&gt;init_flag_mesh&lt;/tt&gt;. The background mesh consists of two large rectangles for the ground and wall, and a thin cylinder with a pointed truck making the flagpole. The wall, ground, and flagpole are assigned texture coordinates to sample out of a single &lt;b&gt;texture atlas&lt;/b&gt; image containing the grass, brick, and metal textures, stored in &lt;tt&gt;background.tga&lt;/tt&gt;. This allows the entire background to be rendered in a single pass with the same active texture. The flagpole is additionally given a yellow specular color, which will give it a metallic sheen when we shade it. The flag is generated by evaluating the function &lt;tt&gt;calculate_flag_vertex&lt;/tt&gt; at regular intervals between zero and one on the &lt;i&gt;s&lt;/i&gt; and &lt;i&gt;t&lt;/i&gt; parametric axes, generating something that looks sort of like a flag flapping in the breeze. The flag being a separate mesh makes it easy to update the mesh data as the flag animates, and lets us render it with its own texture, loaded from &lt;tt&gt;flag.tga&lt;/tt&gt;.&lt;/p&gt;
&lt;h3&gt;Streaming dynamic mesh data&lt;/h3&gt;
&lt;pre&gt;void update_flag_mesh(
    struct flag_mesh const *mesh,
    struct flag_vertex *vertex_data,
    GLfloat time
) {
    GLsizei s, t, i;
    for (t = 0, i = 0; t &amp;lt; FLAG_Y_RES; ++t)
        for (s = 0; s &amp;lt; FLAG_X_RES; ++s, ++i) {
            GLfloat ss = FLAG_S_STEP * s, tt = FLAG_T_STEP * t;

            calculate_flag_vertex(&amp;vertex_data[i], ss, tt, time);
        }

    glBindBuffer(GL_ARRAY_BUFFER, mesh-&gt;vertex_buffer);
    glBufferData(
        GL_ARRAY_BUFFER,
        FLAG_VERTEX_COUNT * sizeof(struct flag_vertex),
        vertex_data,
        GL_STREAM_DRAW
    );
}&lt;/pre&gt;
&lt;p&gt;To animate the flag, we use our &lt;tt&gt;glutIdleFunc&lt;/tt&gt; callback to recalculate the flag's vertices and update the contents of the vertex buffer. We update the buffer with the same &lt;tt&gt;glBufferData&lt;/tt&gt; function we used to initialize it. However, both on initialization and on each update, we give the flag vertex data the &lt;tt&gt;GL_STREAM_DRAW&lt;/tt&gt; hint instead of the &lt;tt&gt;GL_STATIC_DRAW&lt;/tt&gt; hint we've been using until now. This tells the OpenGL driver to optimize for the fact that we'll be continuously replacing the buffer with new data. Since only the positions and normals of the vertices themselves changes, the element buffer for the flag can remain static. The connectivity of the vertices doesn't change.&lt;/p&gt;
&lt;h3&gt;Using a depth buffer to order 3D objects&lt;/h3&gt;
&lt;p&gt;Since we're drawing multiple objects in &lt;span class="smallcap"&gt;3d&lt;/span&gt; space, we need to ensure that objects closer to the viewer render on top of the objects behind them. An easy way to do this would be to just render the objects back-to-front&amp;mdash;in our case, render the background mesh first, then the flag on top of it&amp;mdash;but this is inefficient because of the &lt;b&gt;overdraw&lt;/b&gt; this approach leads to: fragments get generated and processed by the fragment shader for background objects, only to be immediately overwritten by the foreground objects in front of it. Back-to-front rendering also cannot render mutually overlapping objects, such as two interlocked rings, on its own, for rendering either object first will cause it to entirely overlap the other.&lt;/p&gt;
&lt;p&gt;Graphics cards use &lt;b&gt;depth buffers&lt;/b&gt; to provide efficient and reliable ordering of &lt;span class="smallcap"&gt;3d&lt;/span&gt; objects. A depth buffer is a part of the framebuffer that sits alongside the color buffer, and like the color buffer, is a two-dimensional array of pixel values. Instead of color values, the depth buffer stores a depth value, associating a projection-space &lt;i&gt;z&lt;/i&gt; coordinate to each pixel. When a triangle is rasterized with &lt;b&gt;depth testing&lt;/b&gt; enabled, each fragment's projected &lt;i&gt;z&lt;/i&gt; value is compared to the &lt;i&gt;z&lt;/i&gt; value currently stored in the depth buffer. If the fragment would be further away from the viewer than the current depth buffer value, the fragment is discarded. Otherwise, the fragment gets rendered to the color and depth buffers, the new &lt;i&gt;z&lt;/i&gt; value replacing the old depth buffer value.&lt;/p&gt;
&lt;p&gt;In addition to providing correct ordering of objects, depth buffering also minimizes the cost of overdraw if you render objects front-to-back. Although the rasterizer will still generate fragments for parts of objects obscured by already-rendered objects, modern GPUs can discard these obscured fragments before they get run through the fragment shader, reducing the number of overall fragment shader invocations the processor needs to execute. Since our flag mesh appears in front of the background mesh, we thus render the flag before the background so that the obscured parts of the background don't need to be shaded.&lt;/p&gt;
&lt;p&gt;To use depth testing in our program, we need to ask for a depth buffer in our framebuffer and then enable depth testing in the OpenGL state. With GLUT, we can ask for a depth buffer for a window by passing the &lt;tt&gt;GLUT_DEPTH&lt;/tt&gt; flag to &lt;tt&gt;glutInitDisplayMode&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;
int main(int argc, char* argv[])
{
    glutInit(&amp;argc, argv);
    glutInitDisplayMode(GLUT_RGB | GLUT_DEPTH | GLUT_DOUBLE);
    /* ... */
}
&lt;/pre&gt;
&lt;p&gt;We enable and disable depth testing by calling &lt;tt&gt;glEnable&lt;/tt&gt; or &lt;tt&gt;glDisable&lt;/tt&gt; with &lt;tt&gt;GL_DEPTH_TEST&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;static void init_gl_state(void)
{
    /* ... */
    glEnable(GL_DEPTH_TEST);
    /* ... */
}&lt;/pre&gt;
&lt;p&gt;When we start rendering our scene, we need to clear the depth buffer along with the color buffer to ensure that stale depth values don't affect rendering. We can clear both buffers with a single &lt;tt&gt;glClear&lt;/tt&gt; call by passing it both &lt;tt&gt;GL_COLOR_BUFFER_BIT&lt;/tt&gt; and &lt;tt&gt;GL_DEPTH_BUFFER_BIT&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;static void render(void)
{
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    /* ... */
}&lt;/pre&gt;
&lt;h3&gt;Back-face culling&lt;/h3&gt;
&lt;p&gt;Another potential source of overdraw comes from within an object. If you look at the cylindrical flagpole from any direction, you're going to see at most half of its surface. The front-facing triangles appear in front of the back-facing triangles, but they rasterize into the same pixels on screen. Depending on the ordering of triangles in the mesh, the front-facing triangles will either overdraw the back-facing triangles or the fragments of the back-facing triangles will fail the depth test, requiring some extra work from the GPU in either case.&lt;/p&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl4-back-face-culling-01.png"&gt;
&lt;p&gt;However, we can get the GPU to cheaply and quickly discard back-facing triangles even before they get rasterized or depth-tested. If we enable &lt;b&gt;back-face culling&lt;/b&gt;, the graphics card will classify every triangle as front- or back-facing after running the vertex shader and immediately prior to rasterization, completely discarding back-facing triangles. It does this by looking at the &lt;b&gt;winding&lt;/b&gt; of each triangle in projection space. By default, triangles winding counterclockwise are considered front-facing. This works because transforming a triangle to face the opposite direction from the viewer reverses its winding. By constructing our meshes so that all of the triangles wind counterclockwise when viewed from the front, we can use back-face culling to eliminate most of the work of rasterizing those triangles when they face away from the viewer. Only the vertex shader will need to run for their vertices.&lt;/p&gt;
&lt;p&gt;Back-face culling is enabled and disabled by passing &lt;tt&gt;GL_CULL_FACE&lt;/tt&gt; to &lt;tt&gt;glEnable&lt;/tt&gt;/&lt;tt&gt;glDisable&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;static void init_gl_state(void)
{
    /* ... */
    glEnable(GL_CULL_FACE);
    /* ... */
}&lt;/pre&gt;
&lt;h3&gt;Updating the projection matrix and viewport&lt;/h3&gt;
&lt;p&gt;If you go back a chapter and try resizing the &lt;tt&gt;hello-gl&lt;/tt&gt; window, you'll notice that the image stretches to fit the new size of the window, ruining the aspect ratio we worked so hard to preserve. In order to maintain an accurate aspect ratio, we have to recalculate our projection matrix when the window size changes, taking the new aspect ratio into account. We also have to inform OpenGL of the new viewport size by calling &lt;tt&gt;glViewport&lt;/tt&gt;. GLUT allows us to provide a callback that gets invoked when the window is resized using &lt;tt&gt;glutReshapeFunc&lt;/tt&gt;:
&lt;pre&gt;static void reshape(int w, int h)
{
    g_resources.window_size[0] = w;
    g_resources.window_size[1] = h;
    update_p_matrix(g_resources.p_matrix, w, h);
    glViewport(0, 0, w, h);
}&lt;/pre&gt;
&lt;pre&gt;int main(int argc, char* argv[])
{
    /* ... */
    glutReshapeFunc(&amp;reshape);
    /* ... */
}&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;update_p_matrix&lt;/tt&gt; function implements the perspective matrix formula from last chapter and stores the new projection matrix in the &lt;tt&gt;g_resources.p_matrix&lt;/tt&gt; array, from which we'll feed our shaders' &lt;tt&gt;p_matrix&lt;/tt&gt; uniform variable.&lt;/p&gt;
&lt;h3&gt;Handling mouse and keyboard input with GLUT&lt;/h3&gt;
&lt;p&gt;GLUT provides extremely primitive support for mouse and keyboard input. In &lt;tt&gt;flag&lt;/tt&gt;, I've made it so that dragging the mouse moves the view around, and the view snaps back to its original position when the mouse button is released. GLUT offers a &lt;tt&gt;glutMotionFunc&lt;/tt&gt; callback that gets called when the mouse moves while a button is held down and a &lt;tt&gt;glutMouseFunc&lt;/tt&gt; that gets called when a mouse button is pressed or released. (There's also &lt;tt&gt;glutPassiveMotionFunc&lt;/tt&gt; to handle mouse motion when a button isn't pressed, which we don't use.) Our &lt;tt&gt;glutMotionFunc&lt;/tt&gt; adjusts the model-view matrix relative to the distance from the center of the window, and our &lt;tt&gt;glutMouseFunc&lt;/tt&gt; resets it when the mouse button is let go:&lt;/p&gt;
&lt;pre&gt;static void drag(int x, int y)
{
    float w = (float)g_resources.window_size[0];
    float h = (float)g_resources.window_size[1];
    g_resources.eye_offset[0] = (float)x/w - 0.5f;
    g_resources.eye_offset[1] = -(float)y/h + 0.5f;
    update_mv_matrix(g_resources.mv_matrix, g_resources.eye_offset);
}

static void mouse(int button, int state, int x, int y)
{
    if (button == GLUT_LEFT_BUTTON &amp;&amp; state == GLUT_UP) {
        g_resources.eye_offset[0] = 0.0f;
        g_resources.eye_offset[1] = 0.0f;
        update_mv_matrix(g_resources.mv_matrix, g_resources.eye_offset);
    }
}&lt;/pre&gt;
&lt;pre&gt;int main(int argc, char* argv[])
{
    /* ... */
    glutMotionFunc(&amp;drag);
    glutMouseFunc(&amp;mouse);
    /* ... */
}&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;update_mv_matrix&lt;/tt&gt; function is similar to &lt;tt&gt;update_p_matrix&lt;/tt&gt;. It generates a translation matrix, following the formula from last chapter, and stores it to &lt;tt&gt;g_resources.mv_matrix&lt;/tt&gt;, from which we feed the shaders' &lt;tt&gt;mv_matrix&lt;/tt&gt; uniform variable.&lt;/p&gt;
&lt;p&gt;I also rigged &lt;tt&gt;flag&lt;/tt&gt; so you can reload the GLSL program from disk while the demo is running by pressing the &lt;tt&gt;R&lt;/tt&gt; key. The &lt;tt&gt;glutKeyboardFunc&lt;/tt&gt; callback gets called when a key is pressed. Our callback checks if the pressed key was &lt;tt&gt;R&lt;/tt&gt;, and if so, calls &lt;tt&gt;update_flag_program&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;static void keyboard(unsigned char key, int x, int y)
{
    if (key == 'r' || key == 'R') {
        update_flag_program();
    }
}&lt;/pre&gt;
&lt;pre&gt;int main(int argc, char* argv[])
{
    /* ... */
    glutKeyboardFunc(&amp;keyboard);
    /* ... */
}&lt;/pre&gt;
&lt;p&gt;&lt;tt&gt;update_flag_program&lt;/tt&gt; attempts to load, compile, and link the &lt;tt&gt;flag.v.glsl&lt;/tt&gt; and &lt;tt&gt;flag.f.glsl&lt;/tt&gt; files from disk, and if successful, replaces the old shader and program objects.&lt;/p&gt;

&lt;p&gt;That covers the C code for the &lt;tt&gt;flag&lt;/tt&gt; demo. The actual shading happens inside the GLSL code, which we'll look at next.&lt;/p&gt;
&lt;h3&gt;Phong shading&lt;/h3&gt;
&lt;p&gt;Physically accurate light simulation requires expensive algorithms that have only recently become possible for even high-end computer clusters to calculate in real time. Fortunately, human eyes don't require perfect physical accuracy, especially not for fast-moving animated graphics, and real-time computer graphics has come a long way rendering impressive graphics on typical consumer hardware using cheap tricks that approximate the behavior of light without simulating it perfectly. The most fundamental of these tricks is the &lt;b&gt;Phong shading model&lt;/b&gt;, an inexpensive approximation of how light interacts with simple materials developed by computer graphics pioneer Bui Tuong Phong in the early 1970s. Phong shading is a &lt;b&gt;local illumination&lt;/b&gt; simulation&amp;mdash;it only considers the direct interaction between a light source and a single point. Because of this, Phong shading alone cannot calculate effects that involve the influence of other objects in a scene, such as shadows and mirror reflections. This is why the flag casts no shadow on the ground or wall behind it.&lt;/p&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl4-phong-01.png"&gt;
&lt;p&gt;
The Phong model involves three different lighting terms:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;ambient&lt;/b&gt; reflection, a constant term that simulates the background level of light;&lt;/li&gt;
&lt;li&gt;&lt;b&gt;diffuse&lt;/b&gt; reflection, which gives the material what we usually think of as its color;&lt;/li&gt;
&lt;li&gt;and &lt;b&gt;specular&lt;/b&gt; reflection, the shine of polished or metallic surfaces.
&lt;/ul&gt;
&lt;h3&gt;Diffuse and ambient reflection&lt;/h3&gt;
&lt;p&gt;If you hold a flat sheet of paper up to a lamp in a dark room, it will appear brightest when it faces the lamp head-on, and appear dimmer as you rotate it away from the light, reaching its darkest when it's perpendicular to the light. Curved surfaces behave the same way; if you roll up or crumple the paper, its surface will be brightest where it faces the light the most directly. The wider the angle between the surface normal and the light direction, the darker the paper appears. If the paper and light remain stationary but you move your head, the paper's apparent color and brightness won't change. Likewise, in the &lt;tt&gt;flag&lt;/tt&gt; demo, if you drag the view with the mouse, you can see the flag's shading remains the same. The surface reflects light evenly in every direction, or "diffusely." This basic lighting effect is thus called &lt;b&gt;diffuse reflection&lt;/b&gt;.&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl4-phong-diffuse-01.png"&gt;&lt;/center&gt;
&lt;p&gt;
There's an inexpensive operation called the &lt;a href="http://en.wikipedia.org/wiki/Dot_product"&gt;&lt;b&gt;dot product&lt;/b&gt;&lt;/a&gt; that produces a scalar value from two vectors related to the angle between them. Given two unit vectors &lt;i&gt;u&lt;/i&gt; and &lt;i&gt;v&lt;/i&gt;, if their dot product &lt;i&gt;u &amp;#xb7; v&lt;/i&gt; (pronounced "u dot v") is one, then the vectors face the exact same direction; if zero, they're perpendicular; and if negative one, they face exact opposite directions. Positive dot products indicate acute angles while negative dot products indicate obtuse angles. GLSL provides a function &lt;tt&gt;dot(u,v)&lt;/tt&gt; to calculate the dot product of two same-sized &lt;tt&gt;vec&lt;/tt&gt; values.
&lt;p&gt;
The dot product's behavior follows that of diffuse reflection: surfaces reflect more light the more parallel to a light source they become, or in other words, the closer the dot product of their normal and the light's direction gets to one. Perpendicular or back-facing surfaces reflect no light, and their dot product will be zero or negative. This relationship between the dot product and diffuse brightness was first observed by 18th-century physicist Johann Lambert and is referred to as &lt;a href="http://en.wikipedia.org/wiki/Lambertian_reflectance"&gt;&lt;b&gt;Lambertian reflectance&lt;/b&gt;&lt;/a&gt;, and surfaces that exhibit the behavior are called &lt;b&gt;Lambertian surfaces&lt;/b&gt;. Phong shading uses Lambertian reflectance to model diffuse reflection, taking the dot product of the surface normal and the direction from the surface to the light source. If the dot product is greater than zero, it is multiplied by the diffuse color of the light, and the result is multiplied with the surface diffuse color to get the shaded result. (Multiplying two color values involves multiplying their corresponding red, green, blue, and alpha components together, which is what GLSL's &lt;tt&gt;*&lt;/tt&gt; operator does when given two &lt;tt&gt;vec4&lt;/tt&gt;s.) If the dot product is zero or negative, the diffuse color will be zero.
&lt;/p&gt;
&lt;p&gt;
However, in the real world, even when a surface isn't directly lit, it still won't appear pitch black. In any enclosed area, there will be a certain amount of &lt;b&gt;ambient reflection&lt;/b&gt; bouncing around, dimly illuminating areas that the light sources don't directly hit. The Phong model simulates the ambient effect by assigning light sources a constant ambient color. This ambient color gets added to the light's diffuse color after it's been multiplied by the dot product. The sum of ambient and diffuse effect colors is then multiplied by the surface's diffuse color to give the shaded result.
&lt;/p&gt;

&lt;h3&gt;Specular reflection&lt;/h3&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl4-phong-specular-01.png"&gt;&lt;/center&gt;
&lt;p&gt;Not all surfaces reflect light uniformly; many materials, including metals, glass, hair, and skin, have a reflective sheen. Unlike with diffuse reflection, if the viewer moves while a light source and shiny object remain stationary, the shine will move along the surface with the viewer. You can see this simulated in the &lt;tt&gt;flag&lt;/tt&gt; demo by looking at the flagpole: as you drag the view up and down, the gold sheen moves along the pole with you. Physically, an object appears shiny when its surface is covered in highly reflective &lt;b&gt;microfacets&lt;/b&gt;. These facets face every direction, creating a bright shiny spot where the light source reflects directly toward the viewer. This effect is called &lt;b&gt;specular reflection&lt;/b&gt;.&lt;/p&gt;
&lt;p&gt;The specular effect is caused by reflection from the light source to the viewer, so Phong shading simulates the specular effect by reflecting the light direction around the surface normal to give a reflection direction. We can then take the dot product of the reflection direction and the direction from the surface to the viewer. Microfacets on a specular surface follow a &lt;b&gt;normal distribution&lt;/b&gt;: a plurality of facets lie parallel to the surface, and there is an exponential dropoff in the number of facets at steeper angles from the surface. The dropoff is sharper for more polished surfaces, giving a smaller, tighter specular highlight. Phong shading approximates this distribution by raising the dot product to an exponent called the &lt;b&gt;shininess factor&lt;/b&gt;, with higher shininess giving a more polished shine and lower factors giving a more diffuse sheen. This final specular factor is then multiplied by the specular colors of the light source and surface, and the result added to the diffuse and ambient colors to give the final color. Non-specular surfaces have a transparent specular color with red, green, blue, and alpha components set to zero, which eliminates the specular term from the shading equation.&lt;/p&gt;

&lt;h3&gt;Implementing Phong shading in GLSL&lt;/h3&gt;
&lt;p&gt;
Shading calculations are usually performed in the vertex and fragment shaders, where they can leverage the GPU's parallel processing power. (This is where the term "shader" for GPU programs comes from.) Let's bring back the graphics pipeline diagram to get an overview of the Phong shading dataflow:
&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl4-pipeline-01.png"&gt;&lt;/center&gt;
&lt;p&gt;For the best accuracy, we perform shading at a per-fragment level. (For better performance, shading can also be done in the vertex shader and the results interpolated between vertices, but this will lead to less accurate shading, especially for specular effects.) The vertex shader, &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/flag.v.glsl"&gt;&lt;tt&gt;flag.v.glsl&lt;/tt&gt;&lt;/a&gt;, thus only performs transformation and projection, using the &lt;tt&gt;p_matrix&lt;/tt&gt; and &lt;tt&gt;mv_matrix&lt;/tt&gt; we pass in as uniforms. The shader forwards most of the material vertex attributes to varying variables for the fragment shader to use:
&lt;/p&gt;
&lt;pre&gt;#version 110

uniform mat4 p_matrix, mv_matrix;
uniform sampler2D texture;

attribute vec3 position, normal;
attribute vec2 texcoord;
attribute float shininess;
attribute vec4 specular;

varying vec3 frag_position, frag_normal;
varying vec2 frag_texcoord;
varying float frag_shininess;
varying vec4 frag_specular;

void main()
{
    vec4 eye_position = mv_matrix * vec4(position, 1.0);
    gl_Position = p_matrix * eye_position;
    frag_position = eye_position.xyz;
    frag_normal   = (mv_matrix * vec4(normal, 0.0)).xyz;
    frag_texcoord = texcoord;
    frag_shininess = shininess;
    frag_specular = specular;
}&lt;/pre&gt;
&lt;p&gt;In addition to the texture coordinate, shininess, and specular color, the vertex shader also outputs to the fragment shader the model-view-transformed vertex position. The model-view matrix transforms the coordinate space so that the viewer is at the origin, so we can determine the surface-to-viewer direction needed by the specular calculation from ths transformed position. We likewise transform the normal vector to keep it in the same frame of reference as the position. Since the normal is a directional vector without a position, we apply the matrix to it with a &lt;i&gt;w&lt;/i&gt; component of zero, which cancels out the translation of the modelview matrix and only applies its rotation. With this set of varying values, the fragment shader, &lt;a href="http://github.com/jckarter/ch4-flag/blob/master/flag.v.glsl"&gt;&lt;tt&gt;flag.f.glsl&lt;/tt&gt;&lt;/a&gt;, can perform the actual Phong calculation:&lt;/p&gt;
&lt;pre&gt;#version 110

uniform mat4 p_matrix, mv_matrix;
uniform sampler2D texture;

varying vec3 frag_position, frag_normal;
varying vec2 frag_texcoord;
varying float frag_shininess;
varying vec4 frag_specular;

const vec3 light_direction = vec3(0.408248, -0.816497, 0.408248);
const vec4 light_diffuse = vec4(0.8, 0.8, 0.8, 0.0);
const vec4 light_ambient = vec4(0.2, 0.2, 0.2, 1.0);
const vec4 light_specular = vec4(1.0, 1.0, 1.0, 1.0);

void main()
{
    vec3 mv_light_direction = (mv_matrix * vec4(light_direction, 0.0)).xyz,
         normal = normalize(frag_normal),
         eye = normalize(frag_position),
         reflection = reflect(mv_light_direction, normal);

    vec4 frag_diffuse = texture2D(texture, frag_texcoord);
    vec4 diffuse_factor
        = max(-dot(normal, mv_light_direction), 0.0) * light_diffuse;
    vec4 ambient_diffuse_factor
        = diffuse_factor + light_ambient;
    vec4 specular_factor
        = max(pow(-dot(reflection, eye), frag_shininess), 0.0)
            * light_specular;
    
    gl_FragColor = specular_factor * frag_specular
        + ambient_diffuse_factor * frag_diffuse;
}&lt;/pre&gt;
&lt;p&gt;To keep things simple, the shader defines a single light source using &lt;tt&gt;const&lt;/tt&gt; values in the shader source. A real renderer would likely feed these light parameters in as uniform values, so that lights can be moved or their material attributes changed from the host program. With the light attributes embedded in the GLSL as constants, it's easy to change the light attributes in the source, press &lt;tt&gt;R&lt;/tt&gt; to reload the shader, and see the result. Our light source acts as if it were infinitely far away, shining from the same &lt;tt&gt;light_direction&lt;/tt&gt; on every surface in the scene. The light is white, with a 20% baseline ambient light level. It can be made colored by replacing &lt;tt&gt;light_diffuse&lt;/tt&gt;, &lt;tt&gt;light_ambient&lt;/tt&gt;, &lt;tt&gt;light_specular&lt;/tt&gt; with RGBA values.&lt;/p&gt;
&lt;p&gt;The fragment shader uses several new GLSL functions we haven't seen before:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;tt&gt;normalize(v)&lt;/tt&gt; returns a unit vector with the same direction as &lt;tt&gt;v&lt;/tt&gt;. We use it here to convert the fragment position into a direction vector, and to ensure that the normal is a unit vector. Even if our original vertex normals are all unit vectors, their linear interpolations won't be.&lt;/li&gt;
&lt;li&gt;&lt;tt&gt;pow(x,n)&lt;/tt&gt; raises &lt;tt&gt;x&lt;/tt&gt; to the &lt;tt&gt;n&lt;/tt&gt;th power, which we use to apply the specular shininess factor.&lt;/li&gt;
&lt;li&gt;&lt;tt&gt;max(n,m)&lt;/tt&gt; returns the larger of &lt;tt&gt;n&lt;/tt&gt; or &lt;tt&gt;m&lt;/tt&gt;. We use it to clamp dot products less than zero so they shade the same as if they were zero.&lt;/li&gt;
&lt;li&gt;&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl4-reflection-01.png"&gt;&lt;tt&gt;reflect(u,v)&lt;/tt&gt; reflects the vector &lt;tt&gt;u&lt;/tt&gt; around &lt;tt&gt;v&lt;/tt&gt;, giving a vector that makes the same angle with &lt;tt&gt;v&lt;/tt&gt; as &lt;tt&gt;u&lt;/tt&gt;, but in the opposite direction. With it we derive the reflected eye direction for the specular calculation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We transform our constant &lt;tt&gt;light_direction&lt;/tt&gt; to put it in the same coordinate space as the &lt;tt&gt;normal&lt;/tt&gt; and &lt;tt&gt;eye&lt;/tt&gt; vectors. We then sample the surface's diffuse &lt;tt&gt;color&lt;/tt&gt; from the mesh texture We assign the shaded value to &lt;tt&gt;gl_FragColor&lt;/tt&gt; to generate the final shaded fragment.&lt;/p&gt;

&lt;h3&gt;Tweaking the Phong model for stylistic effects&lt;/h3&gt;
&lt;p&gt;Before we wrap things up, let's take a quick look at how the Phong framework can be manipulated to give more stylized results. The classic Phong model is a &lt;b&gt;photorealistic&lt;/b&gt; model: it attempts to model real-world light behavior. But photorealism isn't always desirable. Many games set themselves apart visually by using more stylized shading effects. These effects often use the basic Phong model of diffuse, ambient, and specular lighting, but they warp the individual factors before summing them together.&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl4-diffuse-effects-01.png"&gt;&lt;/center&gt;
&lt;p&gt;As a trivial example, we can get a brighter, softer shading effect if, instead of clamping the diffuse dot product of back-facing surfaces to zero, we scale it so that perpendicular surfaces receive half illumination, and back-facing surfaces scale linearly toward zero. &lt;i&gt;Team Fortress 2&lt;/i&gt; uses this "half Lambert" reflectance scale, so called because the standard Lambertian dropoff rate is halved, as a basis for its cartoonish but semi-photorealistic look (albeit &lt;a href="http://www.valvesoftware.com/publications/2007/NPAR07_IllustrativeRenderingInTeamFortress2.pdf"&gt;heavily modified&lt;/a&gt;). Let's modify &lt;tt&gt;flag.f.glsl&lt;/tt&gt; to warp the diffuse dot product:
&lt;/p&gt;
&lt;pre&gt;float warp_diffuse(float d)
{
    return d * 0.5 + 0.5;
}

void main()
{
    // ...
    vec4 diffuse_factor
        = max(&lt;span class="highlight"&gt;warp_diffuse(&lt;/span&gt;-dot(normal, mv_light_direction)&lt;span class="highlight"&gt;)&lt;/span&gt;, 0.0) * light_diffuse;
    // ...
}

&lt;/pre&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl4-flag-half-lambert-shaded.png"&gt;&lt;/center&gt;
&lt;p&gt;
A popular effect that builds from this half-Lambert scale is &lt;b&gt;cel shading&lt;/b&gt;, in which a stair-step function is applied to the half-Lambert factor so that surfaces are shaded flatly with higher contrast between light and dark areas, in the style of traditional hand-drawn animation cels. &lt;i&gt;Jet Set Radio&lt;/i&gt; pioneered this look, and it's since been used in countless games. Implementing it in GLSL is easy:
&lt;/p&gt;
&lt;pre&gt;float cel(float d)
{
    return smoothstep(0.35, 0.37, d) * 0.4 + smoothstep(0.70, 0.72, d) * 0.6;
}

float warp_diffuse(float d)
{
    return cel(d * 0.5 + 0.5);
}&lt;/pre&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl4-flag-cel-shaded.png"&gt;&lt;/center&gt;
&lt;p&gt;GLSL's &lt;tt&gt;smoothstep(lo,hi,x)&lt;/tt&gt; function behaves like this: if &lt;tt&gt;x&lt;/tt&gt; is less than &lt;tt&gt;lo&lt;/tt&gt;, it returns &lt;tt&gt;0.0&lt;/tt&gt;; if greater than &lt;tt&gt;hi&lt;/tt&gt;, it returns &lt;tt&gt;1.0&lt;/tt&gt;; if in between, it transitions linearly from zero to one. Our &lt;tt&gt;cel&lt;/tt&gt; function above uses &lt;tt&gt;smoothstep&lt;/tt&gt; to create three flat shading levels with short linear transitions in between.&lt;/p&gt;
&lt;p&gt;
There are other effects that can be performed by messing with the &lt;tt&gt;warp_diffuse&lt;/tt&gt; function. For example, the function doesn't need to be &lt;tt&gt;float&lt;/tt&gt;-to-&lt;tt&gt;float&lt;/tt&gt; but could also map to a color scale; you could map greater dot products to warmer reddish colors while lesser products map to cooler bluish colors to give an artistic illustration effect. I encourage you to experiment with the fragment shader code to see what other effects you can create.
&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;With Phong shading implemented, we can start adding additional effects to further improve the look of the flag scene. The most glaring problem is the lack of shadow cast by the flag, so next chapter we'll look at shadow mapping, a technique for rendering accurate shadows into a scene, and learn about off-screen framebuffer objects in the process. Meanwhile, if you're interested in learning more about real-time shading techniques on your own without an OpenGL bias, I highly recommend the book &lt;a href="http://www.realtimerendering.com/"&gt;&lt;i&gt;Real-Time Rendering&lt;/i&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html"&gt;&amp;laquo; Chapter 3&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-07-15T15:06:09+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 3: 3D transformation and projection</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html"&gt;&amp;laquo; Chapter 2.3&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html"&gt;Chapter 4 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
The GPU's specialty, and by extension OpenGL's, is in rendering three-dimensional scenes. If you compare &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;last chapter&lt;/a&gt;'s &lt;tt&gt;hello-gl&lt;/tt&gt; program to, say, Crysis, you might notice that our demo is missing one of those dimensions (among other things). In this chapter, I'm going to fix that. We'll cover the basic math that makes &lt;span class="smallcap"&gt;3d&lt;/span&gt; rendering happen, looking at how &lt;b&gt;transformations&lt;/b&gt; are done using &lt;b&gt;matrices&lt;/b&gt; and how &lt;b&gt;perspective projection&lt;/b&gt; works. Wikipedia does a great job going in-depth about the algorithmic details, so I'm going to spend most of my time talking at a high level about what math we use and why, linking to the relevant Wikipedia articles if you're interested in exploring further. As we look at different transformations, we're going to take the vertex shader from last chapter and extend it to implement those transformations, animating the "hello world" image by moving its rectangle around in &lt;span class="smallcap"&gt;3d&lt;/span&gt; space.
&lt;/p&gt;
&lt;p&gt;Before we start, there are some changes we need to make to last chapter's &lt;tt&gt;hello-gl&lt;/tt&gt; program so that it's easier to play around with. These changes will allow us to write different vertex shaders and supply them as command-line arguments when we run the program, like so:&lt;/p&gt;
&lt;pre&gt;
./hello-gl hello-gl.v.glsl
&lt;/pre&gt;
&lt;p&gt;You can pull these changes from my &lt;a href="http://github.com/jckarter/hello-gl-ch3"&gt;&lt;tt&gt;hello-gl-ch3&lt;/tt&gt; github repo&lt;/a&gt;.
&lt;/p&gt;
&lt;h3&gt;Updating &lt;tt&gt;hello-gl&lt;/tt&gt;&lt;/h3&gt;
&lt;p&gt;We'll start by expanding our vertex array to hold three-dimensional vectors. We'll actually pad them out to four components&amp;mdash;the fourth component's purpose will become clear soon. For now, we'll just set all the fourth components to one. Let's update our vertex array data in &lt;tt&gt;hello-gl.c&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;
static const GLfloat g_vertex_buffer_data[] = { 
    &lt;span class="highlight"&gt;-1.0f, -1.0f, 0.0f, 1.0f,
     1.0f, -1.0f, 0.0f, 1.0f,
    -1.0f,  1.0f, 0.0f, 1.0f,
     1.0f,  1.0f, 0.0f, 1.0f&lt;/span&gt;
};
&lt;/pre&gt;
&lt;p&gt;and our &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt; call:&lt;/p&gt;
&lt;pre&gt;
    glVertexAttribPointer(
        g_resources.attributes.position,  /* attribute */
        &lt;span class="highlight"&gt;4&lt;/span&gt;,                                /* size */
        GL_FLOAT,                         /* type */
        GL_FALSE,                         /* normalized? */
        &lt;span class="highlight"&gt;sizeof(GLfloat)*4&lt;/span&gt;,                /* stride */
        (void*)0                          /* array buffer offset */
    );
&lt;/pre&gt;
&lt;p&gt;When we start transforming our rectangle, it will no longer completely cover the window, so let's add a &lt;tt&gt;glClear&lt;/tt&gt; to our &lt;tt&gt;render&lt;/tt&gt; function so we don't get garbage in the background. We'll set it to dark grey so it's distinct from the black background of our images:
&lt;pre&gt;
static void render(void)
{
    &lt;span class="highlight"&gt;glClearColor(0.1f, 0.1f, 0.1f, 1.0f);
    glClear(GL_COLOR_BUFFER_BIT);&lt;/span&gt;
    /* ... */
}
&lt;/pre&gt;
&lt;p&gt;Now let's generalize a few things. First, we'll change our uniform state to include GLUT's &lt;tt&gt;timer&lt;/tt&gt; value directly rather than the &lt;tt&gt;fade_factor&lt;/tt&gt; precalculated. This will let our new vertex shaders perform additional time-based effects.&lt;/p&gt;
&lt;pre&gt;
static void update_&lt;span class="highlight"&gt;timer&lt;/span&gt;(void)
{
    int milliseconds = glutGet(GLUT_ELAPSED_TIME);
    g_resources.&lt;span class="highlight"&gt;timer&lt;/span&gt; = &lt;span class="highlight"&gt;(float)milliseconds * 0.001f&lt;/span&gt;;
    glutPostRedisplay();
}
&lt;/pre&gt;
&lt;p&gt;You'll also have to search-and-replace all of the other references to &lt;tt&gt;fade_factor&lt;/tt&gt; with &lt;tt&gt;timer&lt;/tt&gt;. Once that's done, we'll change our &lt;tt&gt;main&lt;/tt&gt; and &lt;tt&gt;make_resources&lt;/tt&gt; functions so they can take the vertex shader filename as an argument. This way, we can easily switch between the different vertex shaders we'll be writing:&lt;/p&gt;
&lt;pre&gt;
static int make_resources(&lt;span class="highlight"&gt;const char *vertex_shader_file&lt;/span&gt;)
{
    /* ... */
    g_resources.vertex_shader = make_shader(
        GL_VERTEX_SHADER,
        &lt;span class="highlight"&gt;vertex_shader_file&lt;/span&gt;
    );
    /* ... */
}
&lt;/pre&gt;
&lt;pre&gt;
int main(int argc, char** argv)
{
    /* ... */
    if (!make_resources(&lt;span class="highlight"&gt;argc &gt;= 2 ? argv[1] : "hello-gl.v.glsl"&lt;/span&gt;)) {
        fprintf(stderr, "Failed to load resources\n");
        return 1;
    }
    /* ... */
}
&lt;/pre&gt;
&lt;p&gt;Now let's update our shaders to match our changes to the uniform state and vertex array. We can move the fade factor calculation into the vertex shader, which will pass it on to the fragment shader as a &lt;tt&gt;varying&lt;/tt&gt; value. In &lt;tt&gt;hello-gl.v.glsl&lt;/tt&gt;:&lt;/p&gt;
&lt;pre&gt;
#version 110

&lt;span class="highlight"&gt;uniform float timer;&lt;/span&gt;

attribute &lt;span class="highlight"&gt;vec4&lt;/span&gt; position;

varying vec2 texcoord;
&lt;span class="highlight"&gt;varying float fade_factor;&lt;/span&gt;

void main()
{
    gl_Position = &lt;span class="highlight"&gt;position&lt;/span&gt;;
    texcoord = position&lt;span class="highlight"&gt;.xy&lt;/span&gt; * vec2(0.5) + vec2(0.5);
    &lt;span class="highlight"&gt;fade_factor = sin(timer) * 0.5 + 0.5;&lt;/span&gt;
}
&lt;/pre&gt;
&lt;p&gt;A new feature of GLSL I use here is vector &lt;b&gt;swizzling&lt;/b&gt;: not only can you address the components of a &lt;tt&gt;vec&lt;/tt&gt; type as if they were struct fields by using &lt;tt&gt;.x&lt;/tt&gt;, &lt;tt&gt;.y&lt;/tt&gt;, &lt;tt&gt;.z&lt;/tt&gt;, and &lt;tt&gt;.w&lt;/tt&gt; for the first through fourth components, you can also string together the element letters to collect multiple components in any order into a longer or shorter vector type. &lt;tt&gt;position.xy&lt;/tt&gt; picks out as a &lt;tt&gt;vec2&lt;/tt&gt; the first two elements of our now four-component &lt;tt&gt;position&lt;/tt&gt; vector. We can then feed that &lt;tt&gt;vec2&lt;/tt&gt; into the calculation for our &lt;tt&gt;texcoord&lt;/tt&gt;, which remains two components long.
&lt;/p&gt;
&lt;p&gt;Finally, in &lt;tt&gt;hello-gl.f.glsl&lt;/tt&gt;, we make &lt;tt&gt;fade_factor&lt;/tt&gt; assume its new &lt;tt&gt;varying&lt;/tt&gt; identity:&lt;/p&gt;
&lt;pre&gt;
#version 110

uniform sampler2D textures[2];

&lt;span class="highlight"&gt;varying&lt;/span&gt; float fade_factor;
varying vec2 texcoord;

void main()
{
    gl_FragColor = mix(
        texture2D(textures[0], texcoord),
        texture2D(textures[1], texcoord),
        fade_factor
    );
}
&lt;/pre&gt;
&lt;p&gt;&lt;a name="gl3-math-part"&gt;With those changes out of the way, we can recompile the executable once and not have to mess with C any more for the rest of the chapter. We can write new vertex shader files and execute them using &lt;tt&gt;./hello-gl vertex-shader.v.glsl&lt;/tt&gt; without recompiling anything. Now we're ready do some math!&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Projection and world space&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-projection-space-01.png"&gt;
&lt;p&gt;The destination space for the vertex shader, which I've been informally referring to as "screen space" in the last couple of chapters, is more precisely called &lt;b&gt;projection space&lt;/b&gt;. The visible part of projection space is the unit-radius cube from (&amp;ndash;1, &amp;ndash;1, &amp;ndash;1) to (1, 1, 1). Anything outside of this cube gets &lt;b&gt;clipped&lt;/b&gt; and thrown out. The &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt; axes map across the &lt;b&gt;viewport&lt;/b&gt;, the part of the screen in which any rendered output will be displayed, with (&amp;ndash;1, &amp;ndash;1, &lt;i&gt;z&lt;/i&gt;) corresponding to the lower left corner, (1, 1, &lt;i&gt;z&lt;/i&gt;) to the upper right, and (0, 0, &lt;i&gt;z&lt;/i&gt;) to the center. The rasterizer uses the &lt;i&gt;z&lt;/i&gt; coordinate to assign a depth value to every fragment it generates; if the framebuffer has a depth buffer, these depth values can be compared against the depth values of previously rendered fragments, allowing parts of newly-rendered objects to be hidden behind objects that have already been rendered into the framebuffer. (&lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, &amp;ndash;1) is the &lt;b&gt;near plane&lt;/b&gt; and maps to the nearest depth value. At the other end, (&lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, 1) is the &lt;b&gt;far plane&lt;/b&gt; and maps to the farthest depth value. Fragments with &lt;i&gt;z&lt;/i&gt; coordinates outside of that range get clipped against these planes just like they do the edges of the screen.&lt;/p&gt;
&lt;p&gt;Projection space is computationally convenient for the GPU, but it's not very usable by itself for modeling vertices within a scene. Rather than input projection-space vertices directly to the pipeline, most programs use the vertex shader to &lt;b&gt;project&lt;/b&gt; objects into it. The pre-projection coordinate system used by the program is called &lt;b&gt;world space&lt;/b&gt;, and can be moved, scaled, and rotated relative to projection space in whatever way the program needs. Within world space, objects also need to move around, changing position, orientation, size, and shape. Both of these operations, mapping world space to projection space and positioning objects in world space, are accomplished by performing &lt;b&gt;transformations&lt;/b&gt; with mathematical structures called &lt;b&gt;matrices&lt;/b&gt;.&lt;/p&gt;
&lt;h3&gt;Linear transformations with matrices&lt;/h3&gt;
&lt;p&gt;
&lt;a href="http://en.wikipedia.org/wiki/Linear_transformation"&gt;&lt;b&gt;Linear transformations&lt;/b&gt;&lt;/a&gt; are operations on an object that preserve the relative size and orientation of parts within the object while uniformly changing its overall size or orientation. They include &lt;b&gt;rotation&lt;/b&gt;, &lt;b&gt;scaling&lt;/b&gt;, and &lt;b&gt;shearing&lt;/b&gt;. If you've ever used the "free transform" tool in Photoshop or GIMP, these are the sorts of transformations it performs. You can think of a linear transformation as taking the &lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, and &lt;i&gt;z&lt;/i&gt; axes of your coordinate space and mapping them to a new set of arbitrary axes &lt;i&gt;x'&lt;/i&gt;, &lt;i&gt;y'&lt;/i&gt;, and &lt;i&gt;z'&lt;/i&gt;:
&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl3-linear-transformation-matrices-01.png"&gt;&lt;/center&gt;
&lt;p&gt;
For clarity, the figure is two-dimensional, but the same idea applies to &lt;span class="smallcap"&gt;3d&lt;/span&gt;. To represent a linear transformation numerically, we can take the vector values of those new axes and arrange them into a 3&amp;#xd7;3 &lt;b&gt;matrix&lt;/b&gt;. We can then perform an operation called &lt;a href="http://en.wikipedia.org/wiki/Matrix_multiplication"&gt;&lt;b&gt;matrix multiplication&lt;/b&gt;&lt;/a&gt; to apply a linear transformation to a vector, or to combine two transformations into a single matrix that represents the combined transformation. In standard mathematical notation, matrices are represented so that the axes are represented as columns going left-to-right. In GLSL and in the OpenGL API, matrices are represented as an array of vectors, each vector representing a column in the matrix. In source code, this results in the values looking transposed from their mathematical notation. This is called &lt;b&gt;column-major order&lt;/b&gt; (as opposed to row-major order, in which each vector element of the matrix array would be a row of the matrix). GLSL provides 2&amp;#xd7;2, 3&amp;#xd7;3, and 4&amp;#xd7;4 matrix types named &lt;tt&gt;mat2&lt;/tt&gt; through &lt;tt&gt;mat4&lt;/tt&gt;. It also overloads its multiplication operator for use between &lt;tt&gt;mat&lt;i&gt;n&lt;/i&gt;&lt;/tt&gt; values of the same type, and between &lt;tt&gt;mat&lt;i&gt;n&lt;/i&gt;&lt;/tt&gt;s and &lt;tt&gt;vec&lt;i&gt;n&lt;/i&gt;&lt;/tt&gt;s, to perform matrix-matrix and matrix-vector multiplication.
&lt;/p&gt;
&lt;p&gt;A nice property of linear transformations is that they work well with the rasterizer's linear interpolation. If we transform all of the vertices of a triangle using the same linear transformation, every point on its surface will retain its relative position to the vertices, so textures and other varying values will transform with the vertices they fill out.
&lt;p&gt;
Note that all linear transformations occur relative to the &lt;b&gt;origin&lt;/b&gt;, that is, the (0, 0, 0) point of the coordinate system, which remains constant through a linear transformation. Because of this, moving an object around in space, called &lt;b&gt;translation&lt;/b&gt; in mathematical terms, is &lt;i&gt;not&lt;/i&gt; a linear transformation, and cannot be represented with a 3&amp;#xd7;3 matrix or composed into other 3&amp;#xd7;3 linear transform matrices. We'll see how to integrate translation into transformation matrices shortly. For now, let's try some linear transformations:
&lt;/p&gt;
&lt;h3&gt;Rotation&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-rotation-matrix-01.png"&gt;
&lt;p&gt;
We'll start by writing a shader that spins our rectangle around the &lt;i&gt;z&lt;/i&gt; axis. Using the &lt;tt&gt;timer&lt;/tt&gt; uniform value as a rotation angle, we'll construct a &lt;a href="http://en.wikipedia.org/wiki/Rotation_matrix"&gt;rotation matrix&lt;/a&gt;, using the &lt;tt&gt;sin&lt;/tt&gt; and &lt;tt&gt;cos&lt;/tt&gt; functions to rotate our matrix axes around the unit circle. The shader looks like this; it's in the repo as &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/rotation.v.glsl"&gt;&lt;tt&gt;rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:
&lt;/p&gt;
&lt;pre&gt;
#version 110

uniform float timer;

attribute vec4 position;

varying vec2 texcoord;
varying float fade_factor;

void main()
{
    mat3 rotation = mat3(
        vec3( cos(timer),  sin(timer),  0.0),
        vec3(-sin(timer),  cos(timer),  0.0),
        vec3(        0.0,         0.0,  1.0)
    );
    gl_Position = vec4(rotation * position.xyz, 1.0);
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
}
&lt;/pre&gt;
&lt;p&gt;(I'm going to be listing only the &lt;tt&gt;main&lt;/tt&gt; function of the next few shaders; the &lt;tt&gt;uniform&lt;/tt&gt;, &lt;tt&gt;attribute&lt;/tt&gt;, and &lt;tt&gt;varying&lt;/tt&gt; declarations will all remain the same from here.) With our changes to &lt;tt&gt;hello-gl&lt;/tt&gt; we can run it like so:&lt;/p&gt;
&lt;pre&gt;
./hello-gl rotation.v.glsl
&lt;/pre&gt;
&lt;p&gt;And this is the result:&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-rotation-screenshot.png"&gt;&lt;/center&gt;
&lt;p&gt;
&lt;h3&gt;Scaling to fit the aspect ratio&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-scaling-matrix-01.png"&gt;
&lt;p&gt;
You probably noticed that the rectangle appears to be horizontally distorted as it rotates. This is because our window is wider than it is tall, so the screen distance covered along a unit on the &lt;i&gt;x&lt;/i&gt; axis of projection space is longer than the distance the same unit would cover along the &lt;i&gt;y&lt;/i&gt; axis. The window is 400 pixels wide and 300 pixels high, giving it an &lt;b&gt;aspect ratio&lt;/b&gt; of 4:3 (the width divided by the height). (This will change if we resize the window, but we won't worry about that for now.) We can compensate for this by applying a &lt;a href="http://en.wikipedia.org/wiki/Scaling_(geometry)#Matrix_representation"&gt;scaling matrix&lt;/a&gt; that scales the &lt;i&gt;x&lt;/i&gt; axis by the reciprocal of the aspect ratio, as in &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/window-scaled-rotation.v.glsl"&gt;&lt;tt&gt;window-scaled-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:
&lt;/p&gt;
&lt;pre&gt;
    mat3 window_scale = mat3(
        vec3(3.0/4.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );
    mat3 rotation = mat3(
        vec3( cos(timer),  sin(timer),  0.0),
        vec3(-sin(timer),  cos(timer),  0.0),
        vec3(        0.0,         0.0,  1.0)
    );
    gl_Position = vec4(window_scale * rotation * position.xyz, 1.0);
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
&lt;/pre&gt;
&lt;p&gt;&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-matrix-order-01.png"&gt;Note that the order in which we rotate and scale is important. Unlike scalar multiplication, matrix multiplication is &lt;i&gt;noncommutative&lt;/i&gt;: Changing the order of the arguments gives different results. This should make intuitive sense: "rotate an object, then squish it horizontally" gives a different result from "squish an object horizontally, then rotate it". As matrix math, you write transformation sequences out right-to-left, backwards compared to English: &lt;tt&gt;scale * rotate * vector&lt;/tt&gt; rotates the vector first, whereas &lt;tt&gt;rotate * scale * vector&lt;/tt&gt; scales first.&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-window-scale-rotation-screenshot.png"&gt;&lt;/center&gt;
&lt;p&gt;Now that we've compensated for the distortion of our window's projection space, we've revealed a dirty secret. Our input rectangle is really a square, and it doesn't match the aspect ratio of our image, leaving it scrunched. We need to scale it again outward, this time &lt;i&gt;before&lt;/i&gt; we rotate, as in &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/window-object-scaled-rotation.v.glsl"&gt;&lt;tt&gt;window-object-scaled-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:
&lt;pre&gt;
    mat3 window_scale = mat3(
        vec3(3.0/4.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );
    mat3 rotation = mat3(
        vec3( cos(timer),  sin(timer),  0.0),
        vec3(-sin(timer),  cos(timer),  0.0),
        vec3(        0.0,         0.0,  1.0)
    );
    mat3 object_scale = mat3(
        vec3(4.0/3.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );
    gl_Position = vec4(window_scale * rotation * object_scale * position.xyz, 1.0);
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
&lt;/pre&gt;
&lt;p&gt;(Alternately, we could change our vertex array and apply a scaling transformation to our generated &lt;tt&gt;texcoord&lt;/tt&gt;s. But I promised we wouldn't be changing the C anymore in this chapter.)&lt;/p&gt; With this shader, our rectangle now rotates the way we would expect it to:&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-window-object-scale-rotation-screenshot.png"&gt;&lt;/center&gt;

&lt;h3&gt;Projection and model-view matrices&lt;/h3&gt;
&lt;p&gt;The &lt;tt&gt;window_scale&lt;/tt&gt; matrix conceptually serves a different purpose from the &lt;tt&gt;rotation&lt;/tt&gt; and &lt;tt&gt;object_scale&lt;/tt&gt; matrices. While the latter two matrices set up our input vertices to be where we want them in world space, the &lt;tt&gt;window_scale&lt;/tt&gt; serves to project world space into projection space in a way that gives an undistorted final render. Matrices used to orient objects in world space, like our &lt;tt&gt;rotation&lt;/tt&gt; and &lt;tt&gt;object_scale&lt;/tt&gt; matrices, are called &lt;b&gt;model-view matrices&lt;/b&gt;, because they are used both to transform models and to position them relative to the viewport. The matrix we use to project, in this case &lt;tt&gt;window_scale&lt;/tt&gt;, is called the &lt;b&gt;projection matrix&lt;/b&gt;. Although both kinds of matrix behave the same, and the line drawn between them is mathematically arbitrary, the distinction is useful because a &lt;span class="smallcap"&gt;3d&lt;/span&gt; application will generally only need a few projection matrices that change rarely (usually only if the window size or screen resolution changes). On the other hand, there can be countless model-view matrices for all of the objects in a scene, which will update constantly as the objects animate.

&lt;h3&gt;Orthographic and perspective projection&lt;/h3&gt;
&lt;p&gt;Projecting with a scaling matrix, as we're doing here, produces an &lt;a href="http://en.wikipedia.org/wiki/Orthographic_projection"&gt;orthographic projection&lt;/a&gt;, in which objects in &lt;span class="smallcap"&gt;3d&lt;/span&gt; space are rendered at a constant scale regardless of their distance from the viewport. Orthographic projections are useful for rendering two-dimensional display elements, such as the UI controls of a game or graphics tool, and in modeling applications where the artist needs to see the exact scales of different parts of a model, but they don't adequately present &lt;span class="smallcap"&gt;3d&lt;/span&gt; scenes in a way most viewers expect. To demonstrate this, let's break out of the &lt;span class="smallcap"&gt;2d&lt;/span&gt; plane and alter our shader to rotate the rectangle around the &lt;i&gt;x&lt;/i&gt; axis, as in &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/orthographic-rotation.v.glsl"&gt;&lt;tt&gt;orthographic-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
    const mat3 projection = mat3(
        vec3(3.0/4.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );

    mat3 rotation = mat3(
        vec3(1.0,         0.0,         0.0),
        vec3(0.0,  cos(timer),  sin(timer)),
        vec3(0.0, -sin(timer),  cos(timer))
    );
    mat3 scale = mat3(
        vec3(4.0/3.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );
    gl_Position = vec4(projection * rotation * scale * position.xyz, 1.0);
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
&lt;/pre&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-orthographic-rotation-screenshot.png"&gt;&lt;/center&gt;
&lt;p&gt;With an orthographic projection, the rectangle doesn't very convincingly rotate in &lt;span class="smallcap"&gt;3d&lt;/span&gt; space&amp;mdash;it just sort of accordions up and down. This is because the top and bottom edges of the rectangle remain the same apparent size as they move toward and away from the view. In the real world, objects appear smaller in our field of view proportional to how far from our eyes they are. This effect is called &lt;b&gt;perspective&lt;/b&gt;, and transforming objects to take perspective into account is called &lt;a href="http://en.wikipedia.org/wiki/3D_projection#Perspective_projection"&gt;&lt;b&gt;perspective projection&lt;/b&gt;.&lt;/a&gt; Perspective projection is accomplished by shrinking objects proportionally to their distance from the "eye". An easy way to do this is to divide each point's position by some function of its &lt;i&gt;z&lt;/i&gt; coordinate. Let's arbitrarily decide that zero on the &lt;i&gt;z&lt;/i&gt; axis remains unscaled, and that points elsewhere on the &lt;i&gt;z&lt;/i&gt; axis scale by half their distance from zero. Correspondingly, let's also scale the &lt;i&gt;z&lt;/i&gt; axis by half, so that the end of the rectangle coming toward us doesn't get clipped to the near plane as it gets magnified. We'll end up with the shader code in &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/naive-perspective-rotation.v.glsl"&gt;&lt;tt&gt;naive-perspective-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:
&lt;pre&gt;
    const mat3 projection = mat3(
        vec3(3.0/4.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 0.5)
    );

    mat3 rotation = mat3(
        vec3(1.0,         0.0,         0.0),
        vec3(0.0,  cos(timer),  sin(timer)),
        vec3(0.0, -sin(timer),  cos(timer))
    );
    mat3 scale = mat3(
        vec3(4.0/3.0, 0.0, 0.0),
        vec3(    0.0, 1.0, 0.0),
        vec3(    0.0, 0.0, 1.0)
    );

    vec3 projected_position = projection * rotation * scale * position.xyz;
    float perspective_factor = projected_position.z * 0.5 + 1.0;

    gl_Position = vec4(projected_position/perspective_factor, 1.0);
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
&lt;/pre&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-naive-perspective-screenshot.png"&gt;&lt;/center&gt;
&lt;p&gt;Now the overall shape of the rectangle appears to rotate in perspective, but the texture mapping is all kinky. This is because perspective projection is a &lt;i&gt;nonlinear&lt;/i&gt; transformation&amp;mdash;different parts of the rectangle get scaled differently depending on how far away they are. This interferes with the linear interpolation the rasterizer applies to the texture coordinates across the surface of our triangles. To properly project texture coordinates as well as other varying values in perspective, we need a different approach that takes the rasterizer into account.&lt;/p&gt;

&lt;h3&gt;Homogeneous coordinates&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-homogeneous-coordinates-01.png"&gt;
&lt;p&gt;Directly applying perspective to an object may not be a linear transformation, but the divisor that perspective applies is a linear function of the perspective distance. If we stored the divisor out-of-band as an extra component of our vectors, we could apply perspective as a matrix transformation, and the rasterizer could linearly interpolate texture coordinates correctly before the perspective divisor is applied. This is in fact what that mysterious 1.0 we've been sticking in the fourth component of our vectors is for. The projection space that &lt;tt&gt;gl_Position&lt;/tt&gt; addresses uses &lt;a href="http://en.wikipedia.org/wiki/Homogeneous_coordinates"&gt;&lt;b&gt;homogeneous coordinates&lt;/b&gt;&lt;/a&gt;. That fourth component, labeled &lt;i&gt;w&lt;/i&gt;, divides the &lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, and &lt;i&gt;z&lt;/i&gt; components when the coordinate is projected. In other words, the homogeneous coordinate [&lt;i&gt;x&lt;/i&gt;:&lt;i&gt;y&lt;/i&gt;:&lt;i&gt;z&lt;/i&gt;:&lt;i&gt;w&lt;/i&gt;] projects to the linear coordinate (&lt;i&gt;x&lt;/i&gt;/&lt;i&gt;w&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;/&lt;i&gt;w&lt;/i&gt;, &lt;i&gt;z&lt;/i&gt;/&lt;i&gt;w&lt;/i&gt;). 
&lt;/p&gt;
&lt;p&gt;
With this trick, we can construct a &lt;b&gt;perspective matrix&lt;/b&gt; that maps distances on the &lt;i&gt;z&lt;/i&gt; axis to scales on the &lt;i&gt;w&lt;/i&gt; axis. As I mentioned, the rasterizer also interpolates varying values in homogeneous space, before the coordinates are projected, so texture coordinates and other varying values will blend correctly over perspective-projected triangles using this matrix. The 3&amp;#xd7;3 linear transformation matrices we've covered extend to 4&amp;#xd7;4 easily&amp;mdash;just extend the columns to four components and add a fourth column that leaves the &lt;i&gt;w&lt;/i&gt; axis unchanged. Let's update our vertex shader to use a proper perspective matrix and &lt;tt&gt;mat4&lt;/tt&gt;s to transform our rectangle, as in &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/perspective-rotation.v.glsl"&gt;&lt;tt&gt;perspective-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:
&lt;/p&gt;
&lt;pre&gt;
    const mat4 projection = mat4(
        vec4(3.0/4.0, 0.0, 0.0, 0.0),
        vec4(    0.0, 1.0, 0.0, 0.0),
        vec4(    0.0, 0.0, 0.5, 0.5),
        vec4(    0.0, 0.0, 0.0, 1.0)
    );

    mat4 rotation = mat4(
        vec4(1.0,         0.0,         0.0, 0.0),
        vec4(0.0,  cos(timer),  sin(timer), 0.0),
        vec4(0.0, -sin(timer),  cos(timer), 0.0),
        vec4(0.0,         0.0,         0.0, 1.0)
    );
    mat4 scale = mat4(
        vec4(4.0/3.0, 0.0, 0.0, 0.0),
        vec4(    0.0, 1.0, 0.0, 0.0),
        vec4(    0.0, 0.0, 1.0, 0.0),
        vec4(    0.0, 0.0, 0.0, 1.0)
    );

    gl_Position = projection * rotation * scale * position;
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
&lt;/pre&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-perspective-screenshot.png"&gt;&lt;/center&gt;
&lt;p&gt;The texture coordinates now project correctly with the rectangle as it rotates in perspective.&lt;/p&gt;

&lt;h3&gt;Affine transformations&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl3-translation-matrix-01.png"&gt;
&lt;p&gt;
Homogeneous coordinates let us pull another trick using 4&amp;#xd7;4 matrices. Earlier, I noted that translation cannot be represented in a 3&amp;#xd7;3 linear transformation matrix. While translation can be achieved by simple vector addition, combinations of translations and linear transformations can't be easily composed that way. However, by using the &lt;i&gt;w&lt;/i&gt; axis column of a 4&amp;#xd7;4 matrix to map the &lt;i&gt;w&lt;/i&gt; axis value back onto the &lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, and &lt;i&gt;z&lt;/i&gt; axes, we can set up a &lt;b&gt;translation matrix&lt;/b&gt;. The combination of a linear transformation with a translation is referred to as an &lt;b&gt;affine transformation&lt;/b&gt;. Like our 3&amp;#xd7;3 linear transformation matrices, 4&amp;#xd7;4 affine transformation matrices can be multiplied together to give new matrices combining their transformations.
&lt;/p&gt;

&lt;h3&gt;Constructing a view frustum matrix&lt;/h3&gt;
&lt;p&gt;The perspective projection matrix we constructed above gets the job done, but it's a bit ad-hoc. An easier to understand way of projecting world space would be to consider the origin to be the camera position and project from there. Now that we know how to make translation matrices, we can leave the model-view matrix to position the camera in world space. Different programs will also want to control the &lt;a href="http://en.wikipedia.org/wiki/Angle_of_view"&gt;&lt;b&gt;angle of view&lt;/b&gt;&lt;/a&gt; (&lt;i&gt;&amp;alpha;&lt;/i&gt;) of the projection, and the distance of the near (&lt;i&gt;z&lt;sub&gt;n&lt;/sub&gt;&lt;/i&gt;) and far (&lt;i&gt;z&lt;sub&gt;f&lt;/sub&gt;&lt;/i&gt;) planes in world space. A narrower angle of view will project a far-away object to a scale more similar to close objects, giving a zoomed-in effect, while a wider angle makes objects shrink more relative to their distance, giving a wider field of view. The ratio between the near and far planes affects the resolution of the depth buffer. If the planes are too far apart, or the near plane too close to zero, you'll get &lt;b&gt;z-fighting&lt;/b&gt;, where the &lt;i&gt;z&lt;/i&gt; coordinates of projected triangles differ by less than the depth buffer can represent, and depth testing gives invalid results, causing nearby objects to "fight" for pixels along their shared edge.&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl3-view-frustum-01.png"&gt;&lt;/center&gt;
&lt;p&gt;From these variables, we can come up with a general function to construct a projection matrix for any &lt;b&gt;&lt;a href="http://en.wikipedia.org/wiki/View_frustum"&gt;view frustum&lt;/a&gt;&lt;/b&gt;. The math is a little hairy; I'll describe what it does in broad strokes. With the camera at the origin, we can project the &lt;i&gt;z&lt;/i&gt; axis directly to &lt;i&gt;w&lt;/i&gt; axis values.  In an affine transformation matrix, the bottom row is always set to [0 0 0 1]. This leaves the &lt;i&gt;w&lt;/i&gt; axis unchanged. Changing this bottom row will cause the &lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, or &lt;i&gt;z&lt;/i&gt; axis values to project onto the &lt;i&gt;w&lt;/i&gt; axis, giving a perspective effect along the specified axis. In our case, setting that last row to [0 0 1 0] projects the &lt;i&gt;z&lt;/i&gt; axis value directly to the perspective scale on &lt;i&gt;w&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;We'll then need to remap the range on the &lt;i&gt;z&lt;/i&gt; axis from &lt;i&gt;z&lt;sub&gt;n&lt;/sub&gt;&lt;/i&gt; to &lt;i&gt;z&lt;sub&gt;f&lt;/sub&gt;&lt;/i&gt; so that it projects into the space between the near (&amp;ndash;1) and far (1) planes of projection space. Taking the effect of the &lt;i&gt;w&lt;/i&gt; coordinate into account, we'll have to map into the range from &amp;ndash;&lt;i&gt;z&lt;sub&gt;n&lt;/sub&gt;&lt;/i&gt; (which with a &lt;i&gt;w&lt;/i&gt; coordinate of &lt;i&gt;z&lt;sub&gt;n&lt;/sub&gt;&lt;/i&gt; will project to &amp;ndash;1) to &lt;i&gt;z&lt;sub&gt;f&lt;/sub&gt;&lt;/i&gt; (which with a &lt;i&gt;w&lt;/i&gt; coordinate that's also &lt;i&gt;z&lt;sub&gt;f&lt;/sub&gt;&lt;/i&gt; will project to 1). We do this by translating and scaling the &lt;i&gt;z&lt;/i&gt; axis to fit this new range. The angle of view is determined by how much we scale the &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt; axes. A scale of one gives a 45&amp;deg; angle of view; shrinking the axes gives a wider field of view, and growing them gives a narrower field, inversely proportional to the tangent of the angle of view. So that our output isn't distorted, we also scale the &lt;i&gt;y&lt;/i&gt; axis proportionally to the aspect ratio (&lt;i&gt;r&lt;/i&gt;) of the viewport.
&lt;/p&gt;
&lt;p&gt;
Let's write one last shader using the view frustum matrix function. We'll translate the rectangle to set it 3 units in front of us. In addition to rotating around the &lt;i&gt;x&lt;/i&gt; axis, we'll also change the translation over time to set it moving in a circle left to right and toward and away from us. Here's the code, from &lt;a href="http://github.com/jckarter/hello-gl-ch3/blob/master/view-frustum-rotation.v.glsl"&gt;&lt;tt&gt;view-frustum-rotation.v.glsl&lt;/tt&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
#version 110

uniform float timer;

attribute vec4 position;

varying vec2 texcoord;
varying float fade_factor;

mat4 view_frustum(
    float angle_of_view,
    float aspect_ratio,
    float z_near,
    float z_far
) {
    return mat4(
        vec4(1.0/tan(angle_of_view),           0.0, 0.0, 0.0),
        vec4(0.0, aspect_ratio/tan(angle_of_view),  0.0, 0.0),
        vec4(0.0, 0.0,    (z_far+z_near)/(z_far-z_near), 1.0),
        vec4(0.0, 0.0, -2.0*z_far*z_near/(z_far-z_near), 0.0)
    );
}

mat4 scale(float x, float y, float z)
{
    return mat4(
        vec4(x,   0.0, 0.0, 0.0),
        vec4(0.0, y,   0.0, 0.0),
        vec4(0.0, 0.0, z,   0.0),
        vec4(0.0, 0.0, 0.0, 1.0)
    );
}

mat4 translate(float x, float y, float z)
{
    return mat4(
        vec4(1.0, 0.0, 0.0, 0.0),
        vec4(0.0, 1.0, 0.0, 0.0),
        vec4(0.0, 0.0, 1.0, 0.0),
        vec4(x,   y,   z,   1.0)
    );
}

mat4 rotate_x(float theta)
{
    return mat4(
        vec4(1.0,         0.0,         0.0, 0.0),
        vec4(0.0,  cos(timer),  sin(timer), 0.0),
        vec4(0.0, -sin(timer),  cos(timer), 0.0),
        vec4(0.0,         0.0,         0.0, 1.0)
    );
}

void main()
{
    gl_Position = view_frustum(radians(45.0), 4.0/3.0, 0.5, 5.0)
        * translate(cos(timer), 0.0, 3.0+sin(timer))
        * rotate_x(timer)
        * scale(4.0/3.0, 1.0, 1.0)
        * position;
    texcoord = position.xy * vec2(0.5) + vec2(0.5);
    fade_factor = sin(timer) * 0.5 + 0.5;
}
&lt;/pre&gt;
&lt;p&gt;And this is what we get:&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl3-view-frustum-projection-screenshot.png"&gt;&lt;/center&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;
Matrix multiplication is by far the most common operation in a &lt;span class="smallcap"&gt;3d&lt;/span&gt; rendering pipeline. The rotation, scaling, translation, and frustum matrices we've covered are the basic structures that make &lt;span class="smallcap"&gt;3d&lt;/span&gt; graphics happen. With these fundamentals covered, we're now ready to start building &lt;span class="smallcap"&gt;3d&lt;/span&gt; scenes. If you want to learn more about &lt;span class="smallcap"&gt;3d&lt;/span&gt; math, the book &lt;a href="http://www.amazon.com/Primer-Graphics-Development-Wordware-Library/dp/1556229119"&gt;&lt;i&gt;&lt;span class="smallcap"&gt;3d&lt;/span&gt; Math Primer for Graphics and Game Development&lt;/i&gt;&lt;/a&gt; gives excellent in-depth coverage beyond the basics I've touched on here.
&lt;/p&gt;
&lt;p&gt;In this chapter, I've been demonstrating matrix math by writing code completely within the vertex shader. Constructing our matrices in the vertex shader will cause the matrices to be redundantly calculated for every single vertex. For this simple four-vertex program, it's not a big deal; I stuck to GLSL because it has great support for matrix math built into the language, and demonstrating both the concepts of matrix math and a hoary C math library would make things even more confusing. Unfortunately, OpenGL provides no matrix or vector math through the C API, so we'd need to use a third-party library, such as &lt;a href="http://simdx86.sourceforge.net/"&gt;libSIMDx86&lt;/a&gt;, to perform this math outside of shaders. In a real program with potentially thousands of vertices, the extra matrix math overhead in the vertex shader will add up. Projection matrices generally apply to an entire scene and only need to be recalculated when the window is resized or the screen resolution changed, and model-view matrices usually change only between frames and apply to sets of vertices, so it is more efficient to precalculate these matrices and feed them to the shader as &lt;tt&gt;uniform&lt;/tt&gt;s or &lt;tt&gt;attribute&lt;/tt&gt;s. This is how we'll do things from now on.&lt;/p&gt;
&lt;p&gt;In the next chapter, we'll leave this lame "hello world" program behind and write a program that renders a more sophisticated &lt;span class="smallcap"&gt;3d&lt;/span&gt; scene. In the process, we'll look at the next most important aspect of &lt;span class="smallcap"&gt;3d&lt;/span&gt; rendering after transformation and projection: lighting.
&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html"&gt;&amp;laquo; Chapter 2.3&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html"&gt;Chapter 4 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-07-14T22:34:17+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Table of Contents</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html</link>
<description>
&lt;p&gt;
To make it easier for people jumping into my OpenGL tutorial from the middle, I'm going to keep this post up to date with the new articles as I post them.
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html"&gt;Chapter 1: The Graphics Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;Chapter 2: Hello World: The Slideshow&lt;/a&gt;&lt;/li&gt;
    &lt;ul&gt;
    &lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;Chapter 2.1: Buffers and Textures&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;Chapter 2.2: Shaders&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html"&gt;Chapter 2.3: Rendering&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
&lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html"&gt;Chapter 3: 3D transformation and projection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-4:-Rendering-a-Dynamic-3D-Scene-with-Phong-Shading.html"&gt;Chapter 4: Rendering a Dynamic Scene with Phong Shading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-07-14T22:33:27+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html</guid>
</item>
<item>
<title>An intro to modern OpenGL, in Chinese</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL,-in-Chinese.html</link>
<description>
&lt;p&gt;
&lt;a href="http://blog.csdn.net/kangsongrui/"&gt;Kang Songrui&lt;/a&gt; is in the process of translating my &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;intro to modern OpenGL&lt;/a&gt; articles to Chinese. He recently posted his translation of the &lt;a href="http://blog.csdn.net/kangsongrui/archive/2010/04/25/5527508.aspx"&gt;first chapter&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
As for chapter 4 of the English tutorial, rest assured it's coming. I've had money-making-related projects getting in the way.
&lt;/p&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-26T02:25:51+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL,-in-Chinese.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 2.1: Buffers and Textures</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;&amp;laquo; Chapter 2&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;Chapter 2.2 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;Last time&lt;/a&gt;, we got a window open and awaiting the instructions that will render our hello world program. But before we actually draw anything, we'll need to supply OpenGL with our data by creating &lt;b&gt;objects&lt;/b&gt; of various kinds and uploading data into them. Let's go over the objects we'll need to set up:&lt;/p&gt;
&lt;h3&gt;The pipeline revisited&lt;/h3&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl2-pipeline-01.png"&gt;&lt;/center&gt;
&lt;p&gt;
By walking through &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html#gl1-pipeline"&gt;the graphics pipeline&lt;/a&gt; we went over in the first chapter again, this time from the perspective of our "hello world" program, it will be clear what objects we'll need. Starting from the input end, our vertex array will contain four vertices, which the vertex shader will assign to the corners of the window. The element array will compose these four vertices into a two-triangle strip, making a solid rectangle that covers the window. We will build a couple of small &lt;b&gt;buffer objects&lt;/b&gt; to hold both of these arrays in GPU memory. Our uniform state will consist of our two "hello" images and the fade factor used to blend them. Each of those images will need its own &lt;b&gt;texture object&lt;/b&gt;. In addition to mapping our vertices to the corners of the screen, the vertex shader will assign a set of texture coordinates to each vertex, mapping the vertex to its corresponding corner on the textures. The rasterizer will then interpolate these texture coordinates across the surface of the rectangle so that, finally, our fragment shader can sample the two textures and blend them together using the fade factor. To plug the shaders into OpenGL, we'll create a &lt;b&gt;program object&lt;/b&gt; to link together the vertex and fragment &lt;b&gt;shader objects&lt;/b&gt;. In this article, we'll set up the buffer and texture objects; next time, we'll work on the shaders.
&lt;/p&gt;
&lt;h3&gt;OpenGL C types&lt;/h3&gt;
&lt;p&gt;
OpenGL defines its own set of &lt;tt&gt;GL*&lt;/tt&gt; typedefs that mirrors the standard menagerie of C types: &lt;tt&gt;GLubyte&lt;/tt&gt;, &lt;tt&gt;GLbyte&lt;/tt&gt;, &lt;tt&gt;GLushort&lt;/tt&gt;, &lt;tt&gt;GLshort&lt;/tt&gt;, &lt;tt&gt;GLuint&lt;/tt&gt;, &lt;tt&gt;GLint&lt;/tt&gt;, &lt;tt&gt;GLfloat&lt;/tt&gt;, and &lt;tt&gt;GLdouble&lt;/tt&gt; alias their corresponding C types as you would expect. On top of this basic set of types, OpenGL provides some additional typedefs with more semantic meaning:
&lt;ul&gt;
&lt;li&gt;&lt;tt&gt;GLchar*&lt;/tt&gt;, used by functions that handle strings and expect pointers to null-terminated, ASCII strings
&lt;li&gt;&lt;tt&gt;GLclampf&lt;/tt&gt; and &lt;tt&gt;GLclampd&lt;/tt&gt;, typedefs for &lt;tt&gt;GLfloat&lt;/tt&gt; and &lt;tt&gt;GLdouble&lt;/tt&gt; used when values are expected to be in the range zero to one
&lt;li&gt;&lt;tt&gt;GLsizei&lt;/tt&gt;, an integer typedef suitable for holding the size of a memory buffer, akin to the standard C library's &lt;tt&gt;size_t&lt;/tt&gt;
&lt;li&gt;&lt;tt&gt;GLboolean&lt;/tt&gt;, a typedef for &lt;tt&gt;GLbyte&lt;/tt&gt; intended to contain a &lt;tt&gt;GL_TRUE&lt;/tt&gt; or &lt;tt&gt;GL_FALSE&lt;/tt&gt; value, similar to C++ or C99's &lt;tt&gt;bool&lt;/tt&gt;
&lt;li&gt;&lt;tt&gt;GLenum&lt;/tt&gt;, a typedef of &lt;tt&gt;GLuint&lt;/tt&gt; intended to contain a predefined &lt;tt&gt;GL_*&lt;/tt&gt; constant
&lt;li&gt;&lt;tt&gt;GLbitfield&lt;/tt&gt;, another &lt;tt&gt;GLuint&lt;/tt&gt; typedef intended to contain the bitwise-or of one or more &lt;tt&gt;GL_*_BIT&lt;/tt&gt; masks
&lt;/ul&gt;
&lt;/p&gt;
&lt;h3&gt;Storing our resources&lt;/h3&gt;
&lt;pre&gt;
&lt;a name="gl2-g-resources-buffers-textures"&gt;static struct {
    GLuint vertex_buffer, element_buffer;
    GLuint textures[2];

    /* fields for shader objects ... */
} g_resources;&lt;/a&gt;
&lt;/pre&gt;
&lt;p&gt;A global struct variable like the &lt;tt&gt;g_resources&lt;/tt&gt; struct here is the easiest way to share data between our initialization code and our GLUT callbacks. OpenGL uses opaque &lt;tt&gt;GLuint&lt;/tt&gt; values for object handles. Our &lt;tt&gt;g_resources&lt;/tt&gt; struct contains two &lt;tt&gt;GLuint&lt;/tt&gt; fields we'll use to hold the names of our vertex and element array buffer objects, and a two-element array of &lt;tt&gt;GLuint&lt;/tt&gt;s for our two texture objects. We'll add more fields to hold our shader objects when we construct them in the next article.

&lt;h3&gt;&lt;a name="gl2-object-model"&gt;The OpenGL object model&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;
OpenGL's convention for manipulating objects is a bit unusual. You create objects by generating one or more object &lt;b&gt;names&lt;/b&gt; using a &lt;tt&gt;glGen*s&lt;/tt&gt; function (such as &lt;tt&gt;glGenBuffers&lt;/tt&gt; or &lt;tt&gt;glGenTextures&lt;/tt&gt;). As mentioned above, these names are opaque &lt;tt&gt;GLuint&lt;/tt&gt; values. Any data owned or associated with the object is managed internally by OpenGL. That part's fairly typical. How you use the name is the unusual part: to actually manipulate an object, you first bind its name to an OpenGL-defined &lt;b&gt;target&lt;/b&gt; by calling the corresponding &lt;tt&gt;glBind*&lt;/tt&gt; function (&lt;tt&gt;glBindBuffer&lt;/tt&gt; or &lt;tt&gt;glBindTexture&lt;/tt&gt;). You then provide the &lt;i&gt;target&lt;/i&gt; as an argument to the OpenGL calls that set properties on or upload data into the bound object. Target bindings also affect related OpenGL calls that don't explicitly take the target as a parameter, as we'll see when we discuss rendering. For now, let's see how this pattern plays out when constructing buffer objects:
&lt;/p&gt;
&lt;h3&gt;Buffer objects&lt;/h3&gt;
&lt;pre&gt;
static GLuint make_buffer(
    GLenum target,
    const void *buffer_data,
    GLsizei buffer_size
) {
    GLuint buffer;
    glGenBuffers(1, &amp;buffer);
    glBindBuffer(target, buffer);
    glBufferData(target, buffer_size, buffer_data, GL_STATIC_DRAW);
    return buffer;
}
&lt;/pre&gt;
&lt;p&gt;
Buffer objects are handles to OpenGL-managed memory. Among other things, they are used to store vertex arrays (using the &lt;tt&gt;GL_ARRAY_BUFFER&lt;/tt&gt; target) and element arrays (using the &lt;tt&gt;GL_ELEMENT_ARRAY_BUFFER&lt;/tt&gt; target). When you allocate a buffer with &lt;tt&gt;glBufferData&lt;/tt&gt;, you supply a &lt;b&gt;usage hint&lt;/b&gt; that indicates how often you intend to access and change the data in the buffer, and OpenGL decides the best place in CPU or GPU memory to store its data based on that hint. The hint does not actually constrain how the buffer gets used, but using buffers against their hinted usage will lead to poor performance. For our program, we have constant vertex and element arrays that never need to change, so we give &lt;tt&gt;glBufferData&lt;/tt&gt; the &lt;tt&gt;GL_STATIC_DRAW&lt;/tt&gt; hint. The &lt;tt&gt;STATIC&lt;/tt&gt; part indicates that we don't ever intend to change the data. Buffers can also be hinted either &lt;tt&gt;DYNAMIC&lt;/tt&gt;, which indicates we intend to write into the buffer frequently, or &lt;tt&gt;STREAM&lt;/tt&gt;, which indicates we intend to regularly replace the entire contents of the buffer. The &lt;tt&gt;DRAW&lt;/tt&gt; part indicates that we intend the buffer to be read from only by the GPU. The alternatives to &lt;tt&gt;DRAW&lt;/tt&gt; are &lt;tt&gt;READ&lt;/tt&gt;, which indicates a buffer which will be primarily read back by the CPU, and &lt;tt&gt;COPY&lt;/tt&gt;, which indicates that the buffer will be a conduit between the CPU and GPU and that neither should be given preference. Vertex and element array buffers will almost always use a &lt;tt&gt;GL_*_DRAW&lt;/tt&gt; hint.
&lt;/p&gt;
&lt;pre&gt;
static const GLfloat g_vertex_buffer_data[] = { 
    -1.0f, -1.0f,
     1.0f, -1.0f,
    -1.0f,  1.0f,
     1.0f,  1.0f
};
static const GLushort g_element_buffer_data[] = { 0, 1, 2, 3 };
&lt;/pre&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-vertex-array-01.png"&gt;
&lt;p&gt;&lt;tt&gt;glBufferData&lt;/tt&gt; sees your source data much as &lt;tt&gt;memcpy&lt;/tt&gt; does: just a dumb stream of bytes. We don't tell OpenGL the structure of our arrays until we actually render from them. This allows buffers to store vertex attributes and other data in almost any format, or to feed the same data in different ways to different render jobs. In our case, we just specify the corners of our rectangle as a set of four two-component vectors. Our element array is also simple, an array of &lt;tt&gt;GLushort&lt;/tt&gt;s indexing the four vertex elements in order so that they can be assembled as a rectangular triangle strip. In desktop OpenGL, an element array can consist of 8-bit &lt;tt&gt;GLubyte&lt;/tt&gt;, 16-bit &lt;tt&gt;GLushort&lt;/tt&gt;, or 32-bit &lt;tt&gt;GLuint&lt;/tt&gt; indices; for OpenGL ES, only &lt;tt&gt;GLubyte&lt;/tt&gt; or &lt;tt&gt;GLushort&lt;/tt&gt; can be used. We now fill in our &lt;tt&gt;make_resources&lt;/tt&gt; with calls to &lt;tt&gt;make_buffer&lt;/tt&gt; that allocate and fill our buffers as follows:
&lt;/p&gt;
&lt;pre&gt;
&lt;a name="gl2-make-resources-buffers"&gt;static int make_resources(void)
{
    g_resources.vertex_buffer = make_buffer(
        GL_ARRAY_BUFFER,
        g_vertex_buffer_data,
        sizeof(g_vertex_buffer_data)
    );
    g_resources.element_buffer = make_buffer(
        GL_ELEMENT_ARRAY_BUFFER,
        g_element_buffer_data,
        sizeof(g_element_buffer_data)
    );
    /* make &lt;a href="#gl2-make-resources-textures"&gt;textures&lt;/a&gt; and shaders ... */
}
&lt;/a&gt;&lt;/pre&gt;
&lt;h3&gt;Texture objects&lt;/h3&gt;
&lt;pre&gt;
static GLuint make_texture(const char *filename)
{
    GLuint texture;
    int width, height;
    void *pixels = read_tga(filename, &amp;width, &amp;height);

    if (!pixels)
        return 0;
&lt;/pre&gt;
&lt;p&gt;
As I mentioned in the last article, I'm using the &lt;a href="http://en.wikipedia.org/wiki/Truevision_TGA"&gt;TGA format&lt;/a&gt; to store our "hello world" images. I won't waste time going over the parsing code here; it's in &lt;a href="http://github.com/jckarter/hello-gl/blob/master/util.c"&gt;&lt;tt&gt;util.c&lt;/tt&gt;&lt;/a&gt; in the Github repo if you want to see it. TGA's pixel data is stored as a flat, packed, uncompressed array of three-byte &lt;a href="http://en.wikipedia.org/wiki/RGB"&gt;RGB&lt;/a&gt; pixels (actually stored in BGR order), with the pixels ordered starting from the bottom left of the image and working rightward and upward from there. This format is perfect for feeding into OpenGL textures, as we'll see shortly. If reading the image file fails, we return zero, which is the "null object" name that will never be used by a real OpenGL object.
&lt;/p&gt;
&lt;pre&gt;
    glGenTextures(1, &amp;texture);
    glBindTexture(GL_TEXTURE_2D, texture);
&lt;/pre&gt;
&lt;p&gt;
Texture objects provide handles to structured arrays of GPU memory specialized for storing texture data. OpenGL supports several types of textures, each with its own texture target, including &lt;span class="smallcap"&gt;1d&lt;/span&gt; (&lt;tt&gt;GL_TEXTURE_1D&lt;/tt&gt;), &lt;span class="smallcap"&gt;2d&lt;/span&gt; (&lt;tt&gt;GL_TEXTURE_2D&lt;/tt&gt;), and &lt;span class="smallcap"&gt;3d&lt;/span&gt; (&lt;tt&gt;GL_TEXTURE_3D&lt;/tt&gt;) textures. There are also some more specialized texture types we might run into later. &lt;span class="smallcap"&gt;2d&lt;/span&gt; textures are by far the most common kind. Here we generate and bind a &lt;tt&gt;GL_TEXTURE_2D&lt;/tt&gt; for one of our images. Texture objects are distinct from buffer objects, because the GPU handles texture memory very differently from buffer memory:
&lt;/p&gt;
&lt;h3&gt;Texture sampling and texture parameters&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-texcoords-01.png"&gt;
&lt;p&gt;Whereas the vertex array is fed to the vertex shader one element at a time, and there's no way for any execution of the vertex shader to access other elements, a texture makes its entire contents available to any invocation of either the vertex or fragment shaders. Shaders &lt;b&gt;sample&lt;/b&gt; the texture at one or more floating-point &lt;b&gt;texture coordinates&lt;/b&gt;. The elements of the texture array are distributed evenly into &lt;b&gt;texture space&lt;/b&gt;, a square spanning the coordinates (0, 0) to (1, 1) (or a line segment spanning 0&amp;ndash;1 for &lt;span class="smallcap"&gt;1d&lt;/span&gt; textures, or a cube spanning (0, 0, 0)&amp;ndash;(1, 1, 1) for &lt;span class="smallcap"&gt;3d&lt;/span&gt; textures). To distinguish from the &lt;i&gt;x&lt;/i&gt;, &lt;i&gt;y&lt;/i&gt;, &lt;i&gt;z&lt;/i&gt; coordinates of object space, OpenGL labels the axes of texture space &lt;i&gt;s&lt;/i&gt;, &lt;i&gt;t&lt;/i&gt;, and &lt;i&gt;r&lt;/i&gt;. The texture space square is split evenly along these axes into rectangular cells, corresponding to the width and height of the original array. The cell bordering (0, 0) maps to the first element of the texture array, and subsequent elements get distributed to cells rightward and upward across the &lt;i&gt;s&lt;/i&gt; and &lt;i&gt;t&lt;/i&gt; axes. Sampling the texture at the center of one of these cells gives the corresponding element from the texture array.&lt;/p&gt;
&lt;p&gt;Note that the &lt;i&gt;t&lt;/i&gt; axis can be thought of as increasing either upward or downward (or in any direction, really), depending on the representation of the underlying array. The other axes of texture space are similarly arbitrary. Since TGA images store their pixels left-to-right and bottom-to-top, that's how I'm depicting the axes here.&lt;/p&gt;
&lt;pre&gt;
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S,     GL_CLAMP_TO_EDGE);
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T,     GL_CLAMP_TO_EDGE);
&lt;/pre&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-texture-filter-01.png"&gt;
&lt;p&gt;How sampling behaves when a texture is sampled between the centers of texture cells, or at coordinates outside of the zero-to-one range, is controlled by &lt;b&gt;texture parameters&lt;/b&gt;, set by the &lt;tt&gt;glTexParameteri&lt;/tt&gt; function. The parameters &lt;tt&gt;GL_TEXTURE_MIN_FILTER&lt;/tt&gt; and &lt;tt&gt;GL_TEXTURE_MAG_FILTER&lt;/tt&gt; control how in-between sample points are treated when the texture is sampled at a resolution lower and higher than its native resolution, respectively. We set them to &lt;tt&gt;GL_LINEAR&lt;/tt&gt; to tell the GPU to use &lt;b&gt;linear interpolation&lt;/b&gt; to smoothly blend the four elements closest to the sample point. If the user resizes our window, the texture image will then scale smoothly.  Setting the filters to &lt;tt&gt;GL_NEAREST&lt;/tt&gt; would tell the GPU to return the texture element closest to the sample point, leading to blocky, pixelated scaling.
&lt;/p&gt;
&lt;p&gt;
The &lt;tt&gt;GL_TEXTURE_WRAP_S&lt;/tt&gt; and &lt;tt&gt;GL_TEXTURE_WRAP_T&lt;/tt&gt; parameters control how coordinates beyond the zero-to-one range on their respective axes are treated; in our case, we don't plan to sample outside that range, so we use &lt;tt&gt;GL_CLAMP_TO_EDGE&lt;/tt&gt;, which clamps coordinates below zero to zero, and above one to one. A wrap value of &lt;tt&gt;GL_WRAP&lt;/tt&gt; for one or both axes would cause the texture image to be repeated infinitely through texture space along the wrapped axes.
&lt;/p&gt;
&lt;p&gt;
Describing it in abstract, texture sampling might sound like just extremely convoluted &lt;span class="smallcap"&gt;2d&lt;/span&gt; array indexing. It will make more sense if we look at how our fragment shader will wind up sampling the texture:
&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl2-texture-rasterization-01.png"&gt;&lt;/center&gt;
&lt;p&gt;
In our vertex shader, we'll assign the corners of the texture space square to our rectangle's vertices. When the rasterized size of the rectangle matches the size of the texture (that is, when our window is the same size as the image), the centers of the fragments (the crosses in the figure) will line up with the centers of our texture cells (the circles), and the fragment shader will wind up sampling the image pixel-for-pixel, as you see on the left side. If the rectangle's rasterized size doesn't match the texture, each fragment will wind up sampling between the centers of our texture cells, and the linear filtering will ensure we get a smooth gradient between the texture elements, as the right side demonstrates.
&lt;/p&gt;
&lt;h3&gt;Allocating textures&lt;/h3&gt;
&lt;pre&gt;
    glTexImage2D(
        GL_TEXTURE_2D, 0,           /* target, level of detail */
        GL_RGB8,                    /* internal format */
        width, height, 0,           /* width, height, border */
        GL_BGR, GL_UNSIGNED_BYTE,   /* external format, type */
        pixels                      /* pixels */
    );
    free(pixels);
    return texture;
}
&lt;/pre&gt;
&lt;p&gt;The &lt;tt&gt;glTexImage2D&lt;/tt&gt; (or &lt;tt&gt;-1D&lt;/tt&gt; or &lt;tt&gt;-3D&lt;/tt&gt;) function allocates memory for a texture. Textures can have multiple &lt;b&gt;levels of detail&lt;/b&gt;, sampling from a hierarchy of progressively smaller "&lt;a href="http://en.wikipedia.org/wiki/Mipmap"&gt;mipmaps&lt;/a&gt;" when sampled at lower resolutions, but in our case we only supply the base level zero. Unlike &lt;tt&gt;glBufferData&lt;/tt&gt;, &lt;tt&gt;glTexImage2D&lt;/tt&gt; expects all of the format information for the allocated memory to be presented up front. The &lt;b&gt;internal format&lt;/b&gt; tells the GPU how many color components to store per texture element and at what precision. OpenGL supports all sorts of different image formats; I'll only mention what we use here. Our TGA files use 24-bit RGB pixels, in other words, they sport three 8-bit components per pixel. This corresponds to the &lt;tt&gt;GL_RGB8&lt;/tt&gt; internal format. The &lt;b&gt;width&lt;/b&gt; and &lt;b&gt;height&lt;/b&gt; count the number of texture elements along the &lt;i&gt;s&lt;/i&gt; and &lt;i&gt;t&lt;/i&gt; axes. (The &lt;i&gt;border&lt;/i&gt; argument is a relic and should always be zero.) The &lt;b&gt;external format&lt;/b&gt; and &lt;b&gt;type&lt;/b&gt; declare the component order and type of our &lt;b&gt;pixels&lt;/b&gt; argument, which points to &lt;i&gt;width&lt;/i&gt; &amp;#xd7; &lt;i&gt;height&lt;/i&gt; packed texture elements of the specified format. TGA stores its unsigned byte-sized pixel components in BGR order, so we use &lt;tt&gt;GL_BGR&lt;/tt&gt; for the external format and &lt;tt&gt;GL_UNSIGNED_BYTE&lt;/tt&gt; for the component type.
&lt;/p&gt;
&lt;p&gt;
Let's add some &lt;tt&gt;make_texture&lt;/tt&gt; calls to our &lt;tt&gt;make_resources&lt;/tt&gt; function to create our texture objects:
&lt;/p&gt;
&lt;pre&gt;
&lt;a name="gl2-make-resources-textures"&gt;static int make_resources(void)
{
    /* ... make &lt;a href="#gl2-make-resources-buffers"&gt;buffers&lt;/a&gt; */
    g_resources.textures[0] = make_texture("hello1.tga");
    g_resources.textures[1] = make_texture("hello2.tga");

    if (g_resources.textures[0] == 0 || g_resources.textures[1] == 0)
        return 0;
    /* make shaders ... */
}
&lt;/a&gt;&lt;/pre&gt;
&lt;h3&gt;Next time, shaders&lt;/h3&gt;
&lt;p&gt;
We now have our vertex and image data prepped and ready to launch through the graphics pipeline. The next step is to write the shaders that will steer that data through the GPU and land it on the screen. That's what we'll look at in the next part of this chapter.
&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;&amp;laquo; Chapter 2&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;Chapter 2.2 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-25T20:17:27+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 2.3: Rendering</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;&amp;laquo; Chapter 2.2&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html"&gt;Chapter 3 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
At this point in our &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;"hello world" program&lt;/a&gt;, we've &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;loaded our buffers and textures&lt;/a&gt; and &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;compiled and linked our shader program&lt;/a&gt;. The pieces are all finally in place&amp;mdash;let's render our image.
&lt;/p&gt;
&lt;h3&gt;Overview of a rendering job&lt;/h3&gt;
&lt;p&gt;
Rendering potentially takes a lot of parameters. In addition to all of the buffers, textures, shaders, and uniform parameters it may involve, there are dozens of miscellaneous settings I haven't touched on that control how a rendering job behaves. Rather than offer a monolithic "draw" function with all of these flags as arguments, or require you to fill out a struct with dozens of fields, OpenGL's approach is to lay all of these settings out as a state machine. When you bind objects to targets using &lt;tt&gt;glBindTexture&lt;/tt&gt;, &lt;tt&gt;glBindBuffer&lt;/tt&gt;, and their kin, you are not only making the objects available for modification, you are also binding them to the current rendering job's state. There are also state manipulation functions that set the current shader program, assign values to uniform parameters, and describe the structure of the vertex array. When you finally submit a job with &lt;tt&gt;glDrawElements&lt;/tt&gt;, OpenGL takes a snapshot of the current state and adds it to the GPU's command queue, where it will be executed as soon as the GPU is available. Meanwhile, you can change the OpenGL state around and queue up additional jobs without waiting for your previous jobs to finish. Once you're done queueing up jobs, you tell the window system to "swap buffers", which will wait for all of the queued jobs to finish and then commit the result to the screen.
&lt;/p&gt;
&lt;p&gt;Let's start writing the code to set up our rendering job state:&lt;/p&gt;
&lt;h3&gt;Activating the shader program and assigning uniforms&lt;/h3&gt;
&lt;pre&gt;
static void render(void)
{
    glUseProgram(g_resources.program);
&lt;/pre&gt;
&lt;p&gt;We begin by activating our shader program by passing the name of the linked program object to &lt;tt&gt;glUseProgram&lt;/tt&gt;. Once the program is active, we can start assigning values to our uniform variables. If you recall from looking at &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html#gl2-fragment-shader"&gt;our fragment shader source&lt;/a&gt;, we have the &lt;tt&gt;float fade_factor&lt;/tt&gt; and an array of two &lt;tt&gt;sampler2D&lt;/tt&gt;s named &lt;tt&gt;textures&lt;/tt&gt; to assign.&lt;/p&gt;
&lt;pre&gt;
    glUniform1f(g_resources.uniforms.fade_factor, g_resources.fade_factor);
&lt;/pre&gt;
&lt;p&gt;
OpenGL provides a family of &lt;tt&gt;glUniform*&lt;/tt&gt; functions for assigning to uniform variables, with each member corresponding to a possible type for a uniform variable in a GLSL program. These functions all have names of the form &lt;tt&gt;glUniform{dim}{type}&lt;/tt&gt;, where the &lt;tt&gt;dim&lt;/tt&gt; indicates the size of a vector type (&lt;tt&gt;1&lt;/tt&gt; for an &lt;tt&gt;int&lt;/tt&gt; or &lt;tt&gt;float&lt;/tt&gt; uniform, &lt;tt&gt;2&lt;/tt&gt; for a &lt;tt&gt;vec2&lt;/tt&gt;, etc.), and the &lt;tt&gt;type&lt;/tt&gt; indicates the component type: either &lt;tt&gt;i&lt;/tt&gt; for integer, or &lt;tt&gt;f&lt;/tt&gt; for floating-point. Our &lt;tt&gt;fade_factor&lt;/tt&gt; uniform is a simple &lt;tt&gt;float&lt;/tt&gt;, so we assign it by calling &lt;tt&gt;glUniform1f&lt;/tt&gt;, passing in the uniform's location and new value as arguments.&lt;/p&gt;
&lt;pre&gt;
    glActiveTexture(GL_TEXTURE0);
    glBindTexture(GL_TEXTURE_2D, g_resources.textures[0]);
    glUniform1i(g_resources.uniforms.textures[0], 0);

    glActiveTexture(GL_TEXTURE1);
    glBindTexture(GL_TEXTURE_2D, g_resources.textures[1]);
    glUniform1i(g_resources.uniforms.textures[1], 1);
&lt;/pre&gt;
&lt;p&gt;
Assigning textures to &lt;tt&gt;sampler&lt;/tt&gt;s is a bit more complicated. The GPU has a limited number of &lt;b&gt;texture units&lt;/b&gt; that can supply texture data to any one rendering job. We have to bind our texture objects to these texture units, then assign the indexes of the texture units to our &lt;tt&gt;sampler&lt;/tt&gt; uniform variables as if they were &lt;tt&gt;int&lt;/tt&gt; variables. The &lt;tt&gt;GL_TEXTURE_*&lt;/tt&gt; target name we bind to must also correspond to the type of the &lt;tt&gt;sampler&lt;/tt&gt; uniform. In our case, &lt;tt&gt;GL_TEXTURE_2D&lt;/tt&gt; corresponds to the &lt;tt&gt;sampler2D&lt;/tt&gt; type of our &lt;tt&gt;textures&lt;/tt&gt; variable. &lt;tt&gt;glActiveTexture&lt;/tt&gt; sets the active texture unit. &lt;tt&gt;glBindTexture&lt;/tt&gt; takes the active texture unit as an implicit parameter, binding the given texture object to the target on that unit. (Other texture object manipulation functions like &lt;tt&gt;glTexParameteri&lt;/tt&gt; and &lt;tt&gt;glTexImage2D&lt;/tt&gt; also operate on the texture bound to the active texture unit.) Once we've bound the texture unit, we can assign its index to the uniform using &lt;tt&gt;glUniform1i&lt;/tt&gt;.
&lt;/p&gt;
&lt;h3&gt;Setting up the vertex array&lt;/h3&gt;
&lt;pre&gt;
    glBindBuffer(GL_ARRAY_BUFFER, g_resources.vertex_buffer);
    glVertexAttribPointer(
        g_resources.attributes.position,  /* attribute */
        2,                                /* size */
        GL_FLOAT,                         /* type */
        GL_FALSE,                         /* normalized? */
        sizeof(GLfloat)*2,                /* stride */
        (void*)0                          /* array buffer offset */
    );
    glEnableVertexAttribArray(g_resources.attributes.position);
&lt;/pre&gt;
&lt;p&gt;
Next, we tell OpenGL the format of our vertex array. We do this by going through each vertex attribute and calling &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt;, which associates a part of a vertex buffer with the attribute, and &lt;tt&gt;glEnableVertexAttribArray&lt;/tt&gt;, which tells OpenGL to read values for that attribute from the vertex array while rendering. &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt; takes as arguments the attribute location, the size and component type of the associated attribute variable (for our &lt;tt&gt;position&lt;/tt&gt; attribute, size &lt;tt&gt;2&lt;/tt&gt; and type &lt;tt&gt;GL_FLOAT&lt;/tt&gt;), the number of bytes between attribute values (called the &lt;b&gt;stride&lt;/b&gt;), and the offset of the first attribute value inside the currently bound &lt;tt&gt;GL_ARRAY_BUFFER&lt;/tt&gt;. For historic reasons, the offset is passed as a pointer, but the parameter is used for its integer value, so we pass an integer cast to &lt;tt&gt;void*&lt;/tt&gt;.
&lt;/p&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-vertex-attrib-array-01.png"&gt;
&lt;p&gt;In our case, our vertex array consists only of the single &lt;tt&gt;vec2 position&lt;/tt&gt; attribute; if we had multiple attributes, the attributes' values could be either interleaved, like an array of &lt;tt&gt;struct&lt;/tt&gt;s, or kept in separate arrays. The flexibility &lt;tt&gt;glVertexAttribPointer&lt;/tt&gt; gives in letting us choose the stride and offset of each attribute can accommodate either arrangement. Different attributes can even be potentially read from separate buffer objects; changing the &lt;tt&gt;GL_ARRAY_BUFFER&lt;/tt&gt; binding won't affect the buffer used by attribute array pointers that have already been set.
&lt;/p&gt;
&lt;p&gt;(The &lt;i&gt;normalized?&lt;/i&gt; argument I skipped mentioning above is used with arrays of integers in the vertex array. If true, the components will be mapped from the range of their integer type, such as 0&amp;ndash;255 for an unsigned byte, to the floating-point range 0.0&amp;ndash;1.0, like color components in an image. If false, their integer values will be preserved. For components like ours that are already floating-point, it doesn't have any effect.)&lt;/p&gt;
&lt;/p&gt;

&lt;h3&gt;Submitting the rendering job&lt;/h3&gt;
&lt;pre&gt;
    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, g_resources.element_buffer);
    glDrawElements(
        GL_TRIANGLE_STRIP,  /* mode */
        4,                  /* count */
        GL_UNSIGNED_SHORT,  /* type */
        (void*)0            /* element array buffer offset */
    );
&lt;/pre&gt;
&lt;p&gt;
&lt;tt&gt;glDrawElements&lt;/tt&gt; is the function that sets the graphics pipeline in motion. We tell it what triangle assembly mode we want, how many vertices to assemble triangles from, the type of the components of our element array, and the offset within the currently bound &lt;tt&gt;GL_ELEMENT_ARRAY_BUFFER&lt;/tt&gt; of the first component to render, again as a fake-pointer-but-really-integer. It will then take the pointed-to element array indexes, gather them up with the currently bound shader program, uniform values, texture units, and vertex attribute pointers we just set up, bundle everything into a rendering job, and place the job in the GPU's queue.
&lt;/p&gt;

&lt;h3&gt;Cleaning up after ourselves&lt;/h3&gt;
&lt;pre&gt;
    glDisableVertexAttribArray(g_resources.attributes.position);
&lt;/pre&gt;
&lt;p&gt;"Always leave things the way you found them," the late Bill Brasky once advised. A downside of OpenGL's state machine model is that all of these bindings and settings persist globally, even after &lt;tt&gt;glDrawElements&lt;/tt&gt; is called. This means that we have to take some care with how our OpenGL code will interact with all of the other OpenGL code throughout the program. While there is no other OpenGL code yet in this program to interact with, we should still start learning good habits. Particular care is needed with vertex attributes: In a complex program involving multiple shader programs and multiple vertex arrays, an incorrectly enabled vertex attribute array could potentially cause &lt;tt&gt;glDrawElements&lt;/tt&gt; to try to feed the GPU invalid data, leading to corrupted output or segfaults. It's a good idea to keep a vertex array attribute enabled only as long as it's needed. Here, we disable the vertex attribute array for &lt;tt&gt;position&lt;/tt&gt;.
&lt;/p&gt;
&lt;p&gt;
You might also be thinking, we're rebinding all of the same objects, setting all of the same uniform values (aside from the &lt;tt&gt;fade_factor&lt;/tt&gt;), and reactivating all of the same vertex attributes every time we render. If the state settings persist across &lt;tt&gt;glDrawElements&lt;/tt&gt; calls, we could technically do without almost all of this per-frame setup, getting away with binding everything once before entering &lt;tt&gt;glutMainLoop&lt;/tt&gt; and having &lt;tt&gt;render&lt;/tt&gt; only update the fade factor and call &lt;tt&gt;glDrawElements&lt;/tt&gt;. But again, it's a good idea to set up all the state you expect at the point you expect it. Depending on bits of OpenGL state to remain unchanged between frames is an easy way to breed bugs as your programs grow.
&lt;/p&gt;

&lt;h3&gt;Displaying our finished scene&lt;/h3&gt;
&lt;pre&gt;
    glutSwapBuffers();
}
&lt;/pre&gt;
&lt;p&gt;
We only have the one rendering job to wait on, so now that we've submitted the job and tidied up, we can sync immediately. The GLUT function &lt;tt&gt;glutSwapBuffers&lt;/tt&gt; waits for all running jobs to finish, then swaps the color buffers of our double-buffered framebuffer, moving the currently visible buffer to the "back" to be rendered into by the next frame, and pushing the image we just rendered to the "front", showing the newly-rendered scene in our window. Our rendering is done!
&lt;/p&gt;

&lt;h3&gt;Animating the scene&lt;/h3&gt;
&lt;pre&gt;
static void update_fade_factor(void)
{
    int milliseconds = glutGet(GLUT_ELAPSED_TIME);
    g_resources.fade_factor = sinf((float)milliseconds * 0.001f) * 0.5f + 0.5f;
    glutPostRedisplay();
}
&lt;/pre&gt;
&lt;p&gt;
To keep the image moving, our &lt;tt&gt;glutIdleFunc&lt;/tt&gt; callback continuously updates the value we assign to the &lt;tt&gt;fade_factor&lt;/tt&gt; uniform. GLUT maintains a millisecond timer we can access with &lt;tt&gt;glutGet(GLUT_ELAPSED_TIME)&lt;/tt&gt;; we just feed this through the standard C &lt;tt&gt;sinf&lt;/tt&gt; function to get a smooth, periodic fade from zero to one and back. Every time we update the fade factor, we call &lt;tt&gt;glutPostRedisplay&lt;/tt&gt;, which forces our &lt;tt&gt;render&lt;/tt&gt; callback to get invoked again, updating the window.
&lt;/p&gt;

&lt;h3&gt;Compiling and running the program, again&lt;/h3&gt;
&lt;p&gt;It's finally time to compile and run the whole program with all of our new code. The command to build the executable will look much as it did &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html#gl2-compiling"&gt;last time when we built the dummied-out version&lt;/a&gt;, but this time, you'll build from the real &lt;tt&gt;hello-gl.c&lt;/tt&gt; and &lt;tt&gt;util.c&lt;/tt&gt; source files. If you use the Makefiles, you can build with the default target:&lt;/p&gt;
&lt;pre&gt;
make -f Makefile.MacOSX # or Makefile.Unix or Makefile.Mingw
&lt;/pre&gt;
&lt;pre&gt;
nmake /f Nmakefile.Windows
&lt;/pre&gt;
&lt;p&gt;Once built, the program assumes that all its image and shader resources are in the current directory, so it will work best to run it from the command line from inside the directory containing the executable, the image files, and the shader source. We can finally bask in the glory of our hard work:&lt;/p&gt;
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl2-screenshot.png"&gt;&lt;/center&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;That was admittedly a long way to go for a simple "hello world". But the framework we've built up here is actually pretty flexible; you could swap in your own images and tweak the shaders to transform or filter the images further before sampling them, all without recompiling the C. In the next chapter, we'll mess around with the vertex shader to demonstrate the basics of &lt;span class="smallcap"&gt;3d&lt;/span&gt; transformation and projection.&lt;/p&gt;
&lt;p&gt;If you're interested in breaking off on your own at this point and looking over the &lt;a href="http://www.opengl.org/registry/"&gt;OpenGL specifications&lt;/a&gt; yourself, note that the OpenGL 2 specs still include all of the deprecated features I've been avoiding discussing. I would highly recommend looking instead at the spec for OpenGL 3.1 or later, being sure to look at the "core profile" specs rather than the ones for the "compatibility profiles". While OpenGL 3 and later add a lot of new features over OpenGL 2, all of the basic APIs I've gone over here for OpenGL 2 still form the basis for newer versions.
&lt;p&gt;&lt;a href="http://www.khronos.org/registry/gles/"&gt;OpenGL ES&lt;/a&gt; 2 is also worth looking at. It consists mostly of the forward-looking subset of OpenGL 2 I've been covering; all of the OpenGL APIs I've mentioned are present in OpenGL ES 2 as well. OpenGL ES also adds additional features for mobile platforms, such as fixed-point numeric support and offline shader compilation, that the desktop version of the spec doesn't offer. If you want to try your hand at OpenGL ES development, it is part of the Android NDK and iPhone SDKs, among other mobile development platforms. On Windows, Google's &lt;a href="http://code.google.com/p/angleproject/"&gt;ANGLE project&lt;/a&gt; also provides an implementation of OpenGL ES 2 built on top of DirectX.&lt;/p&gt;
&lt;p&gt;Hopefully this chapter has given you a good taste of the OpenGL API and GLSL language. As always, if you felt something I touched on bears clarification, or there's something in particular you'd like to see me cover, &lt;script&gt;male_to('com', 'joe', 'duriansoftware', 'drop me a line')&lt;/script&gt;.
&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html"&gt;&amp;laquo; Chapter 2.2&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-3:-3D-transformation-and-projection.html"&gt;Chapter 3 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-25T20:13:38+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 2: Hello World: The Slideshow</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html"&gt;&amp;laquo; Chapter 1&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;Chapter 2.1 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
&lt;b&gt;Updates:&lt;/b&gt; I've fixed a few problems people have reported, particularly problems building on Unix and using Visual C++. This chapter also got featured &lt;a href="http://www.reddit.com/r/programming/comments/biw8t/an_intro_to_modern_opengl_chapter_2_hello_world/"&gt;on Reddit&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
In the &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html"&gt;previous chapter&lt;/a&gt; I gave a big-picture overview of the graphics pipeline. Now it's time to put it into action. Before we try rendering any fancy &lt;span class="smallcap"&gt;3d&lt;/span&gt; scenes, I'll follow standard tutorial protocol and use a simple, two-dimensional "hello world" app to demonstrate the basics of the OpenGL API. We're going to take this image:
&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl2-hello-1.png"&gt;&lt;/center&gt;
&lt;p&gt;
and draw it to an appropriately-sized window. But static images are kind of dull&amp;mdash;how about we make it a little more interesting by fading back and forth with this image:
&lt;/p&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl2-hello-2.png"&gt;&lt;/center&gt;
&lt;p&gt;
Still not all that exciting a program, but despite its simplicity, the program will exercise almost all the parts of OpenGL a more complex program would. The completed source code is up on Github &lt;a href="http://github.com/jckarter/hello-gl"&gt;here&lt;/a&gt;. At 380 lines of C and a couple dozen lines of shader code, this program may seem like overkill just to draw an image to the screen. However, much of it will lay the groundwork for the more interesting demos to come. The &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl.c"&gt;&lt;tt&gt;hello-gl.c&lt;/tt&gt;&lt;/a&gt; source file contains the OpenGL rendering bits, while &lt;a href="http://github.com/jckarter/hello-gl/blob/master/util.c"&gt;&lt;tt&gt;util.c&lt;/tt&gt;&lt;/a&gt; contains boring utility functions for reading TGA image files. I've included the two images, &lt;tt&gt;hello1.tga&lt;/tt&gt; and &lt;tt&gt;hello2.tga&lt;/tt&gt;, in this format, because it's easy to parse without depending on an external library. Our shader code lives in two files: &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl.v.glsl"&gt;&lt;tt&gt;hello-gl.v.glsl&lt;/tt&gt;&lt;/a&gt; for the vertex shader, and &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl.f.glsl"&gt;&lt;tt&gt;hello-gl.f.glsl&lt;/tt&gt;&lt;/a&gt; for the fragment shader.
&lt;/p&gt;
&lt;p&gt;
In this chapter, I'll explain how the different parts of the &lt;tt&gt;hello-gl&lt;/tt&gt; program use the OpenGL API to feed data into the graphics pipeline and put it in action. I'll also give a brief overview of the GLSL language when we look at shaders. It's a lot to cover all in one blog post, so I'll break the chapter up into four parts. In this first part, we'll get a window open with GLUT. In the second part, we'll set up the buffer and texture objects that will contain the raw vertex and image data for our program. After that, we'll write the shader code that will process that data into our final image on screen, and then feed the shader into OpenGL. In the final article, we'll go through the OpenGL calls that actually render to the screen. Now that our game plan's laid out, let's start putting the players on the field. We'll start things off by setting up GLUT and getting an empty window up on the screen.
&lt;/p&gt;
&lt;h3&gt;OpenGL header files&lt;/h3&gt;
&lt;pre&gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;GL/glew.h&amp;gt;
#ifdef __APPLE__
#  include &amp;lt;GLUT/glut.h&amp;gt;
#else
#  include &amp;lt;GL/glut.h&amp;gt;
#endif
&lt;/pre&gt;
&lt;p&gt;Different platforms keep their OpenGL headers in different places, but with GLEW, you don't need to worry about that. Including &lt;tt&gt;GL/glew.h&lt;/tt&gt; will pull in the system OpenGL headers for you, wherever they may live. Unfortunately, including GLUT still requires you to manually step around some cross-platform landmines. Its header traditionally lives in &lt;tt&gt;GL/glut.h&lt;/tt&gt;, but MacOS X's bundled GLUT framework uses Apple's own header file convention, putting the GLUT header in &lt;tt&gt;GLUT/glut.h&lt;/tt&gt;. There's also a bug in the way recent versions of Visual Studio's standard C headers interact with &lt;tt&gt;glut.h&lt;/tt&gt; that requires &lt;tt&gt;stdlib.h&lt;/tt&gt; to be included before it.
&lt;/p&gt;

&lt;h3&gt;Setting up our window with GLUT&lt;/h3&gt;
&lt;pre&gt;
int main(int argc, char** argv)
{
    glutInit(&amp;argc, argv);
    glutInitDisplayMode(GLUT_RGB | GLUT_DOUBLE);
    glutInitWindowSize(400, 300);
    glutCreateWindow("Hello World");
    glutDisplayFunc(&amp;render);
    glutIdleFunc(&amp;update_fade_factor);
&lt;/pre&gt;
&lt;p&gt;
GLUT provides a limited, but straightforward and portable, interface to the window system. After prepping GLUT by calling &lt;tt&gt;glutInit&lt;/tt&gt;, we use &lt;tt&gt;glutInitDisplayMode&lt;/tt&gt; to specify what buffers our default framebuffer should have. In our case, a color buffer (&lt;tt&gt;GLUT_RGB&lt;/tt&gt;) with double buffering (&lt;tt&gt;GLUT_DOUBLE&lt;/tt&gt;) is sufficient. (&lt;a href="http://en.wikipedia.org/wiki/Double_buffering#Double_buffering_in_computer_graphics"&gt;Double buffering&lt;/a&gt; provides two color buffers to the framebuffer, alternating which buffer is displayed onscreen and which buffer is drawn into every frame so that animation appears smooth.) If we needed a depth or stencil buffer, we could also ask for them here. We then set the initial size for our window to the 400&amp;#xd7;300 size of our images with &lt;tt&gt;glutInitWindowSize&lt;/tt&gt; and create the window with &lt;tt&gt;glutCreateWindow&lt;/tt&gt;. Finally, we designate two callbacks to receive window events: a &lt;tt&gt;glutDisplayFunc&lt;/tt&gt; to render our image when the window needs displaying, and a &lt;tt&gt;glutIdleFunc&lt;/tt&gt; to continuously update the fade factor between the two images over time.
&lt;/p&gt;
&lt;pre&gt;
    glewInit();
    if (!GLEW_VERSION_2_0) {
        fprintf(stderr, "OpenGL 2.0 not available\n");
        return 1;
    }
&lt;/pre&gt;
&lt;p&gt;
After GLUT creates our window, it prepares OpenGL so that we can start making calls into the library. The first thing we do is initialize GLEW. When &lt;tt&gt;glewInit&lt;/tt&gt; is called, it sets a bunch of flags based on what extensions and OpenGL versions are available. We check the &lt;tt&gt;GLEW_VERSION_2_0&lt;/tt&gt; flag here to ensure we have OpenGL 2.0 available before proceeding. Besides the version flags it sets, GLEW's role is mostly invisible, and we won't need to interact with it after it's been initialized.
&lt;/p&gt;
&lt;pre&gt;
    if (!make_resources()) {
        fprintf(stderr, "Failed to load resources\n");
        return 1;
    }

    glutMainLoop();
    return 0;
}
&lt;/pre&gt;
&lt;p&gt;With GLEW initialized, we call our &lt;tt&gt;make_resources&lt;/tt&gt; function to set up our OpenGL resources. We'll build up that function over the next few parts of this chapter. If our resources load successfully, &lt;tt&gt;glutMainLoop&lt;/tt&gt; takes over. It displays the window, starts receiving UI events from the window system, and invokes the callbacks we set up in response to those events. It will also exit the program for us when the user quits. The &lt;tt&gt;return 0&lt;/tt&gt; merely suppresses compiler warnings and never actually gets reached.
&lt;/p&gt;
&lt;h3&gt;Compiling and running our program&lt;/h3&gt;
&lt;p&gt;
At this point, we can stub out our GLUT callbacks and &lt;tt&gt;make_resources&lt;/tt&gt; function and get a working, if pointless, program:
&lt;/p&gt;
&lt;pre&gt;
static int make_resources(void)
{
    return 1;
}
&lt;/pre&gt;
&lt;pre&gt;
static void update_fade_factor(void)
{
}
&lt;/pre&gt;
&lt;pre&gt;
static void render(void)
{
    glClearColor(1.0f, 1.0f, 1.0f, 1.0f);
    glClear(GL_COLOR_BUFFER_BIT);
    glutSwapBuffers();
}
&lt;/pre&gt;
&lt;p&gt;
&lt;a name="gl2-compiling"&gt;&lt;tt&gt;glClearColor&lt;/tt&gt; sets an &lt;a href="http://en.wikipedia.org/wiki/RGBA"&gt;RGBA&lt;/a&gt; clear color (in this case, white), which &lt;tt&gt;glClear&lt;/tt&gt; then uses to fill the framebuffer's color buffer. &lt;tt&gt;glutSwapBuffers&lt;/tt&gt; then brings our cleared color buffer to the screen. With these stubs in place, we can now compile and run our program. This stubbed-out version is in the Github repo as &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl-dummy.c"&gt;&lt;tt&gt;hello-gl-dummy.c&lt;/tt&gt;&lt;/a&gt;. The command to compile the program and link it to the OpenGL, GLUT, and GLEW libraries will vary across platforms. On most Unixes it should look something like this:
&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
gcc -o hello-gl-dummy hello-gl-dummy.c \
    -I/usr/X11R6/include -L/usr/X11R6/lib \
    -lGL -lGLEW -lglut
&lt;/pre&gt;
&lt;p&gt;On MacOS X:&lt;/p&gt;
&lt;pre&gt;
# Assuming GLEW was installed to /opt/local
gcc -o hello-gl-dummy hello-gl-dummy.c \
    -I/opt/local/include -L/opt/local/lib \
    -framework OpenGL -framework GLUT -lGLEW
&lt;/pre&gt;
&lt;p&gt;On Windows with Visual C++:&lt;/p&gt;
&lt;pre&gt;
cl /Fohello-gl-dummy.obj /c hello-gl-dummy.c
link /out:hello-gl-dummy.exe hello-gl-dummy.obj \
    opengl32.lib glut32.lib glew32.lib
&lt;/pre&gt;
&lt;p&gt;On Windows with mingw:&lt;/p&gt;
&lt;pre&gt;
gcc -o hello-gl-dummy.exe hello-gl-dummy.c \
    -lopengl32 -lglut32 -lglew32
&lt;/pre&gt;
&lt;p&gt;
The repo also includes makefiles for each of these platform groups. You can build this version of the program using the &lt;tt&gt;hello-gl-dummy&lt;/tt&gt; (or &lt;tt&gt;hello-gl-dummy.exe&lt;/tt&gt; on Windows):
&lt;/p&gt;
&lt;pre&gt;
make -f Makefile.MacOSX hello-gl-dummy # or Makefile.Unix or Makefile.Mingw
&lt;/pre&gt;
&lt;pre&gt;
nmake /f Nmakefile.Windows hello-gl-dummy.exe
&lt;/pre&gt;
&lt;p&gt;Once you've built the program, you should then be able to run the program and get a white window, as promised:
&lt;center&gt;&lt;img src="http://duriansoftware.com/joe/media/gl2-dummy-screenshot.png"&gt;&lt;/center&gt;
Close the window, or on MacOS X quit the application, to dismiss it.
&lt;/p&gt;
&lt;h3&gt;Next time, buffers and textures&lt;/h3&gt;
&lt;p&gt;
With the red tape of getting a window open out of the way, we're ready to actually feed our vertexes and images into OpenGL. In the next article, I'll introduce OpenGL's buffer and texture objects.
&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html"&gt;&amp;laquo; Chapter 1&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;Chapter 2.1 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;


</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-19T08:03:35+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 2.2: Shaders</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;&amp;laquo; Chapter 2.1&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html"&gt;Chapter 2.3 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;Buffers and textures&lt;/a&gt; contain the raw materials for an OpenGL program, but without shaders, they are inert lumps of bytes. If you recall from our overview of the &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html#gl1-pipeline"&gt;graphics pipeline&lt;/a&gt;, rendering requires a vertex shader, which maps our vertices into screen space, and a fragment shader, which colors in the rasterized fragments of the resulting triangles. Shaders in OpenGL are written in a language called GLSL (GL Shading Language), which looks a lot like C. In this article, we'll lay out the shader code for our &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;"hello world" program&lt;/a&gt; and then write the C code to load, compile, and link it into OpenGL.
&lt;/p&gt;
&lt;h3&gt;The vertex shader&lt;/h3&gt;
&lt;p&gt;Here is the GLSL source code for our vertex shader, from &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl.v.glsl"&gt;&lt;tt&gt;hello-gl.v.glsl&lt;/tt&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
#version 110

attribute vec2 position;

varying vec2 texcoord;

void main()
{
    gl_Position = vec4(position, 0.0, 1.0);
    texcoord = position * vec2(0.5) + vec2(0.5);
}
&lt;/pre&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-vertex-shader-01.png"&gt;
&lt;p&gt;I'll summarize what the shader does, then give a little more detail about the GLSL language. The shader first assigns the vertex's screen space position to &lt;tt&gt;gl_Position&lt;/tt&gt;, a predefined variable that GLSL provides for the purpose. In screen space, the coordinates (&amp;ndash;1, &amp;ndash;1) and (1, 1) correspond respectively to the lower-left and upper-right corners of the framebuffer; since our vertex array's vertices already trace that same rectangle, we can directly copy the &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt; components from each vertex's &lt;tt&gt;position&lt;/tt&gt; value as it comes out of the vertex array. &lt;tt&gt;gl_Position&lt;/tt&gt;'s other two vector components are used in depth testing and perspective projection; we'll look at them closer next chapter when we get into &lt;span class="smallcap"&gt;3d&lt;/span&gt; math. For now, we just fill them with their identity values zero and one. The shader then does some math to map our screen-space &lt;tt&gt;position&lt;/tt&gt;s from screen space (&amp;ndash;1 to 1) to texture space (0 to 1) and assigns the result to the vertex's &lt;tt&gt;texcoord&lt;/tt&gt;.
&lt;/p&gt;
&lt;p&gt;Much like C, a GLSL shader starts executing from the &lt;tt&gt;main&lt;/tt&gt; function, which in GLSL's case takes no arguments and returns &lt;tt&gt;void&lt;/tt&gt;. GLSL borrows the C preprocessor syntax for its own directives. The &lt;tt&gt;#version&lt;/tt&gt; directive indicates the GLSL version of the following source code; our &lt;tt&gt;#version&lt;/tt&gt; declares that we're using GLSL 1.10. (GLSL versions are pretty tightly tied to OpenGL versions; 1.10 is the version that corresponds to OpenGL 2.0.) GLSL does away with pointers and most of C's sized numeric types, keeping only the &lt;tt&gt;bool&lt;/tt&gt;, &lt;tt&gt;int&lt;/tt&gt;, and &lt;tt&gt;float&lt;/tt&gt; types in common, but it adds a suite of vector and matrix types up to four components in length. The &lt;tt&gt;vec2&lt;/tt&gt; and &lt;tt&gt;vec4&lt;/tt&gt; types you see here are two- and four-component vectors of &lt;tt&gt;float&lt;/tt&gt;s, respectively. A type name can also be used as a constructor function for that type; you can construct a vector from either a single scalar value, which will be repeated into all the components of the vector, or from a combination of vectors and scalars, whose components will be strung together to form a larger vector. GLSL's math operators and many of its builtin functions are defined on these vector types to do component-wise math. In addition to numeric types, GLSL also supplies special &lt;tt&gt;sampler&lt;/tt&gt; data types for sampling textures, which we'll see in the fragment shader below. These basic types can be aggregated into array and user-defined &lt;tt&gt;struct&lt;/tt&gt; types.
&lt;/p&gt;
&lt;p&gt;
A vertex shader communicates with the surrounding graphics pipeline using specially-declared global variables in the GLSL program. Its inputs come from &lt;tt&gt;uniform&lt;/tt&gt; variables, which supply values from the uniform state, and &lt;tt&gt;attribute&lt;/tt&gt; variables, which supply per-vertex attributes from the vertex array. The shader assigns its per-vertex outputs to &lt;tt&gt;varying&lt;/tt&gt; variables. GLSL predefines some varying variables to receive special outputs used by the graphics pipeline, including the &lt;tt&gt;gl_Position&lt;/tt&gt; variable we used here.
&lt;/p&gt;

&lt;h3&gt;The fragment shader&lt;/h3&gt;
&lt;p&gt;Now let's look at the fragment shader source, from &lt;a href="http://github.com/jckarter/hello-gl/blob/master/hello-gl.f.glsl"&gt;&lt;tt&gt;hello-gl.f.glsl&lt;/tt&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;a name="gl2-fragment-shader"&gt;#version 110

uniform float fade_factor;
uniform sampler2D textures[2];

varying vec2 texcoord;

void main()
{
    gl_FragColor = mix(
        texture2D(textures[0], texcoord),
        texture2D(textures[1], texcoord),
        fade_factor
    );
}&lt;/a&gt;
&lt;/pre&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl2-fragment-shader-01.png"&gt;
&lt;p&gt;In a fragment shader, some things change slightly. &lt;tt&gt;varying&lt;/tt&gt; variables become inputs here: Each varying variable in the fragment shader is linked to the vertex shader's varying variable of the same name, and each invocation of the fragment shader receives a rasterized version of the vertex shader's outputs for that varying variable. Fragment shaders are also given a different set of predefined &lt;tt&gt;gl_*&lt;/tt&gt; variables. &lt;tt&gt;gl_FragColor&lt;/tt&gt; is the most important, a &lt;tt&gt;vec4&lt;/tt&gt; to which the shader assigns the RGBA color value for the fragment. The fragment shader has access to the same set of &lt;tt&gt;uniform&lt;/tt&gt;s as the vertex shader, but cannot declare or access &lt;tt&gt;attribute&lt;/tt&gt; variables.&lt;/p&gt;
&lt;p&gt;Our fragment shader uses GLSL's builtin &lt;tt&gt;texture2D&lt;/tt&gt; function to sample the two &lt;tt&gt;textures&lt;/tt&gt; from uniform state at &lt;tt&gt;texcoord&lt;/tt&gt;. It then calls the builtin &lt;tt&gt;mix&lt;/tt&gt; function to combine the two texture values based on the current value of the uniform &lt;tt&gt;fade_factor&lt;/tt&gt;: zero gives only the sample from the first texture, one gives only the second texture's sample, and values in between give us a blend of the two.&lt;/p&gt;
&lt;p&gt;Now that we've looked over the GLSL shader code, let's jump back into C and load the shaders into OpenGL.&lt;/p&gt;
&lt;h3&gt;Storing our shader objects&lt;/h3&gt;
&lt;pre&gt;
static struct {
    /* ... fields for &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html#gl2-g-resources-buffers-textures"&gt;buffer and texture&lt;/a&gt; objects */
    GLuint vertex_shader, fragment_shader, program;
    
    struct {
        GLint fade_factor;
        GLint textures[2];
    } uniforms;

    struct {
        GLint position;
    } attributes;

    GLfloat fade_factor;
} g_resources;
&lt;/pre&gt;
&lt;p&gt;First, let's add some fields to our &lt;tt&gt;g_resources&lt;/tt&gt; structure to hold the names of our shader objects and program object after we construct them. Like buffers and textures, shader and program objects are named by &lt;tt&gt;GLuint&lt;/tt&gt; handles. We also add some fields to hold the integer &lt;b&gt;locations&lt;/b&gt; that we'll need to reference our shaders' uniform and attribute variables. Finally, we add a field to hold the floating-point value we'll assign to the &lt;tt&gt;fade_factor&lt;/tt&gt; uniform every frame.&lt;/p&gt;
&lt;h3&gt;Compiling shader objects&lt;/h3&gt;
&lt;pre&gt;
static GLuint make_shader(GLenum type, const char *filename)
{
    GLint length;
    GLchar *source = file_contents(filename, &amp;length);
    GLuint shader;
    GLint shader_ok;

    if (!source)
        return 0;
&lt;/pre&gt;
&lt;p&gt;OpenGL compiles shader objects from their GLSL source code and keeps the generated GPU machine code to itself. There is no standard way to precompile a GLSL program into a binary&amp;mdash;you build the shader from source every time. Here we read our shader source out of a separate file, which lets us change the shader source without recompiling our C.&lt;/p&gt;
&lt;pre&gt;
    shader = glCreateShader(type);
    glShaderSource(shader, 1, (const GLchar**)&amp;source, &amp;length);
    free(source);
    glCompileShader(shader);
&lt;/pre&gt;
&lt;p&gt;Shader and program objects deviate from the &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html#gl2-object-model"&gt;&lt;tt&gt;glGen&lt;/tt&gt;-and-&lt;tt&gt;glBind&lt;/tt&gt; protocol&lt;/a&gt; that buffer and texture objects follow. Unlike buffer and texture functions, functions that operate on shaders and programs take the object's integer name directly as an argument. The objects don't need to be bound to any target to be modified. Here, we create a shader object by calling &lt;tt&gt;glCreateShader&lt;/tt&gt; with the shader type (either &lt;tt&gt;GL_VERTEX_SHADER&lt;/tt&gt; or &lt;tt&gt;GL_FRAGMENT_SHADER&lt;/tt&gt;). We then supply an array of one or more pointers to strings of source code to &lt;tt&gt;glShaderSource&lt;/tt&gt;, and tell OpenGL to compile the shader with &lt;tt&gt;glCompileShader&lt;/tt&gt;. This step is analogous to the compilation stage of a C build process; a compiled shader object is akin to a &lt;tt&gt;.o&lt;/tt&gt; or &lt;tt&gt;.obj&lt;/tt&gt; file. Just as in a C project, any number of vertex and fragment shader objects can be linked together into a working program, with each shader object referencing functions defined in the others of the same type, as long as the referenced functions all resolve and a &lt;tt&gt;main&lt;/tt&gt; entry point is provided for both the vertex and fragment shaders.&lt;/p&gt;

&lt;pre&gt;
    glGetShaderiv(shader, GL_COMPILE_STATUS, &amp;shader_ok);
    if (!shader_ok) {
        fprintf(stderr, "Failed to compile %s:\n", filename);
        show_info_log(shader, glGetShaderiv, glGetShaderInfoLog);
        glDeleteShader(shader);
        return 0;
    }
    return shader;
}
&lt;/pre&gt;
&lt;p&gt;
Also just like a C program, a block of shader source code can fail to compile due to syntax errors, references to nonexistent functions, or type mismatches. OpenGL maintains an &lt;b&gt;info log&lt;/b&gt; for every shader object that contains errors or warnings raised by the GLSL compiler. After compiling the shader, we need to check its &lt;tt&gt;GL_COMPILE_STATUS&lt;/tt&gt; with &lt;tt&gt;glGetShaderiv&lt;/tt&gt;. If the compile fails, we display the info log using our &lt;tt&gt;show_info_log&lt;/tt&gt; function and give up. Here's how &lt;tt&gt;show_info_log&lt;/tt&gt; looks:
&lt;/p&gt;
&lt;pre&gt;
static void show_info_log(
    GLuint object,
    PFNGLGETSHADERIVPROC glGet__iv,
    PFNGLGETSHADERINFOLOGPROC glGet__InfoLog
)
{
    GLint log_length;
    char *log;

    glGet__iv(object, GL_INFO_LOG_LENGTH, &amp;log_length);
    log = malloc(log_length);
    glGet__InfoLog(object, log_length, NULL, log);
    fprintf(stderr, "%s", log);
    free(log);
}
&lt;/pre&gt;
&lt;p&gt;We pass in the &lt;tt&gt;glGetShaderiv&lt;/tt&gt; and &lt;tt&gt;glGetShaderInfoLog&lt;/tt&gt; functions as arguments to &lt;tt&gt;show_info_log&lt;/tt&gt; so we can reuse the function for program objects further on. (Those &lt;tt&gt;PFNGL*&lt;/tt&gt; function pointer type names are provided by GLEW.) We use &lt;tt&gt;glGetShaderiv&lt;/tt&gt; with the &lt;tt&gt;GL_INFO_LOG_LENGTH&lt;/tt&gt; parameter to get the length of the info log, allocate a buffer to hold it, and download the contents using &lt;tt&gt;glGetShaderInfoLog&lt;/tt&gt;.&lt;/p&gt;

&lt;h3&gt;Linking program objects&lt;/h3&gt;
&lt;pre&gt;
static GLuint make_program(GLuint vertex_shader, GLuint fragment_shader)
{
    GLint program_ok;

    GLuint program = glCreateProgram();
    glAttachShader(program, vertex_shader);
    glAttachShader(program, fragment_shader);
    glLinkProgram(program);
&lt;/pre&gt;
&lt;p&gt;
If shader objects are the object files of the GLSL build process, then program objects are the finished executables. We create a program object using &lt;tt&gt;glCreateProgram&lt;/tt&gt;, attach shader objects to be linked into it with &lt;tt&gt;glAttachShader&lt;/tt&gt;, and set off the link process with &lt;tt&gt;glLinkProgram&lt;/tt&gt;.
&lt;/p&gt;

&lt;pre&gt;
    glGetProgramiv(program, GL_LINK_STATUS, &amp;program_ok);
    if (!program_ok) {
        fprintf(stderr, "Failed to link shader program:\n");
        show_info_log(program, glGetProgramiv, glGetProgramInfoLog);
        glDeleteProgram(program);
        return 0;
    }
    return program;
}
&lt;/pre&gt;
&lt;p&gt;Of course, linking can also fail, due to functions being referenced but not defined, missing &lt;tt&gt;main&lt;/tt&gt;s, fragment shaders using &lt;tt&gt;varying&lt;/tt&gt; inputs not supplied by the vertex shader, and other reasons analogous to the reasons C programs fail to link. We check the program's &lt;tt&gt;GL_LINK_STATUS&lt;/tt&gt; and dump its info log using &lt;tt&gt;show_info_log&lt;/tt&gt;, this time using the program-specific &lt;tt&gt;glGetProgramiv&lt;/tt&gt; and &lt;tt&gt;glGetProgramInfoLog&lt;/tt&gt; functions.&lt;/p&gt;
&lt;p&gt;Now we can fill in the last part of &lt;tt&gt;make_resources&lt;/tt&gt; that compiles and links our shader program:&lt;/p&gt;
&lt;pre&gt;
static int make_resources(void)
{
    /* make &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html#gl2-make-resources-buffers"&gt;buffers&lt;/a&gt; and &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html#gl2-make-resources-textures"&gt;textures&lt;/a&gt; ... */
    g_resources.vertex_shader = make_shader(
        GL_VERTEX_SHADER,
        "hello-gl.v.glsl"
    );
    if (g_resources.vertex_shader == 0)
        return 0;

    g_resources.fragment_shader = make_shader(
        GL_FRAGMENT_SHADER,
        "hello-gl.f.glsl"
    );
    if (g_resources.fragment_shader == 0)
        return 0;

    g_resources.program = make_program(
        g_resources.vertex_shader,
        g_resources.fragment_shader
    );
    if (g_resources.program == 0)
        return 0;
&lt;/pre&gt;
&lt;h3&gt;Looking up shader variable locations&lt;/h3&gt;
&lt;pre&gt;
    g_resources.uniforms.fade_factor
        = glGetUniformLocation(g_resources.program, "fade_factor");
    g_resources.uniforms.textures[0]
        = glGetUniformLocation(g_resources.program, "textures[0]");
    g_resources.uniforms.textures[1]
        = glGetUniformLocation(g_resources.program, "textures[1]");

    g_resources.attributes.position
        = glGetAttribLocation(g_resources.program, "position");

    return 1;
}
&lt;/pre&gt;
&lt;p&gt;The GLSL linker assigns a &lt;tt&gt;GLint&lt;/tt&gt; &lt;b&gt;location&lt;/b&gt; to every &lt;tt&gt;uniform&lt;/tt&gt; value and vertex shader &lt;tt&gt;attribute&lt;/tt&gt;. Structs and arrays of uniforms or attributes get further broken down, with each field getting its own location assigned. When we render using the program, we'll need to use these integer locations when we assign values to the uniform variables and when we map parts of the vertex array to attributes. Here, we use the functions &lt;tt&gt;glGetUniformLocation&lt;/tt&gt; and &lt;tt&gt;glGetAttribLocation&lt;/tt&gt; to look up these locations, giving them the variable, struct field, or array element name as a string. We then record those locations in our program's &lt;tt&gt;g_resources&lt;/tt&gt; struct. With the program linked and the uniform and attribute locations on record, we are now ready to render using the program.&lt;/p&gt;

&lt;h3&gt;Next time, we render&lt;/h3&gt;
&lt;p&gt;I know I've left you hanging these last couple parts without a complete, working program to run. I'll fix that in the next and final part of this chapter, when we write the code that will actually set the graphics pipeline in motion and render our scene.&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.1:-Buffers-and-Textures.html"&gt;&amp;laquo; Chapter 2.1&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.3:-Rendering.html"&gt;Chapter 2.3 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-06T00:29:36+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2.2:-Shaders.html</guid>
</item>
<item>
<title>An intro to modern OpenGL. Chapter 1: The Graphics Pipeline</title>
<link>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html</link>
<description>
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;Chapter 2 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;
&lt;a href="http://www.opengl.org/"&gt;OpenGL&lt;/a&gt; has been around a long time, and from reading all the accumulated layers of documentation out there on the Internet, it's not always clear what parts are historic and what parts are still useful and supported on modern graphics hardware. It's about time for a new OpenGL introduction that walks through the parts that are still relevant today.
&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Update:&lt;/b&gt; Join the &lt;a href="http://www.reddit.com/r/programming/comments/bhyon/an_intro_to_modern_opengl_chapter_1_the_graphics/"&gt;Reddit discussion&lt;/a&gt;.
&lt;h3&gt;What is OpenGL?&lt;/h3&gt;
&lt;p&gt;
&lt;a href="http://en.wikipedia.org/wiki/OpenGL"&gt;Wikipedia&lt;/a&gt; gives a good overview of the purpose and history of OpenGL, but I'll give a quick summary here. In its modern form, OpenGL is a cross-platform library for interfacing with programmable GPUs for the purpose of rendering real-time &lt;span class="smallcap"&gt;3d&lt;/span&gt; graphics. Its use is common in games, CAD, and data visualization applications. It started in the early '90s as a cross-platform standardization of &lt;a href="http://en.wikipedia.org/wiki/Silicon_Graphics"&gt;SGI&lt;/a&gt;'s proprietary GL ("Graphics Library") that drove the graphics hardware in their high-end workstations. A few years later, &lt;a href="http://en.wikipedia.org/wiki/Quake_(video_game)#GLQuake_and_WinQuake"&gt;GLQuake&lt;/a&gt; and &lt;a href="http://en.wikipedia.org/wiki/3dfx"&gt;3dfx&lt;/a&gt;'s Voodoo graphics accelerators pushed &lt;span class="smallcap"&gt;3d&lt;/span&gt; accelerators into the mainstream, and OpenGL became a standard alongside Microsoft's proprietary &lt;a href="http://en.wikipedia.org/wiki/Microsoft_Direct3D"&gt;Direct&lt;span class="smallcap"&gt;3d&lt;/span&gt;&lt;/a&gt; library for controlling graphics accelerators in consumer PCs. In recent years, the &lt;a href="http://www.khronos.org/"&gt;Khronos group&lt;/a&gt; has taken stewardship of the OpenGL standard, updating it to support the features of modern programmable GPUs, pushing it into the mobile and online domains with &lt;a href="http://www.khronos.org/opengles/"&gt;OpenGL ES&lt;/a&gt; and &lt;a href="http://www.khronos.org/webgl/"&gt;WebGL&lt;/a&gt;, and streamlining it in OpenGL 3 by deprecating the outdated features that cluttered earlier versions of the library.
&lt;/p&gt;
&lt;p&gt;
Another recent development has been the adoption of general purpose GPU (GPGPU) libraries, including nVidia's &lt;a href="http://www.nvidia.com/object/cuda_home_new.html"&gt;CUDA&lt;/a&gt; and Khronos' &lt;a href="http://www.khronos.org/opencl/"&gt;OpenCL&lt;/a&gt;. These libraries implement dialects of C with added data parallelism features, allowing the GPU to be used for general computation without having to work within the graphics-oriented framework of OpenGL. However, these GPGPU frameworks don't replace OpenGL; since their primary purpose is not graphics programming, they only provide access to a GPU's computation units, ignoring its graphics-specific hardware. They can, however, act as accessories to OpenGL. CUDA and OpenCL both can share buffers of GPU memory with OpenGL and pass data between GPGPU programs and the graphics pipeline. GPGPU will be outside the scope of these articles; I'll be focusing on using OpenGL for graphics tasks.
&lt;/p&gt;
&lt;p&gt;
For these tutorials, I'm going to assume you're already a programmer and that you know C, but that you haven't necessarily seen OpenGL or done graphics programming before. Knowing at least some basic algebra and geometry will help a lot. I'm going to cover OpenGL 2.0, and avoid discussing any API features that are deprecated or removed in OpenGL 3 or OpenGL ES. If I write enough chapters, I might talk about some of the new features of OpenGL 3 and 4 after I go through the basics. In addition to OpenGL, I'll be using two helper libraries: &lt;a href="http://www.opengl.org/resources/libraries/glut/"&gt;GLUT&lt;/a&gt; (the GL Utility Toolkit), which provides a cross-platform interface between the window system and OpenGL, and &lt;a href="http://glew.sourceforge.net/"&gt;GLEW&lt;/a&gt; (the GL Extensions Wrangler), which streamlines dealing with different versions of OpenGL and their extensions.
&lt;/p&gt;
&lt;h3&gt;Where do I get OpenGL, GLUT, and GLEW?&lt;/h3&gt;
&lt;p&gt;
OpenGL comes standard in some form or another on MacOS X, Windows, and most Linux distributions. If you want to follow these tutorials, you'll need to ensure your OpenGL implementation supports at least version 2.0. MacOS X's OpenGL implementation always supports OpenGL 2.0, at least in software if the graphics card driver doesn't provide it. On Windows, you're dependent on your graphics card drivers to provide OpenGL 2 or later. You can use RealTech's free &lt;a href="http://www.realtech-vr.com/glview/download.html"&gt;OpenGL Extensions Viewer&lt;/a&gt; to see what OpenGL version your driver supports. nVidia and AMD's OpenGL drivers support at least OpenGL 2.0 on all of their video cards released in the past four years. Users of Intel onboard graphics and older graphics cards are less fortunate. For a fallback, &lt;a href="http://www.mesa3d.org/"&gt;Mesa&lt;/a&gt; provides an open-source, cross-platform software OpenGL 2.1 implementation that works on Windows and almost all Unix platforms.
&lt;/p&gt;
&lt;p&gt;Mesa is also the most common OpenGL implementation on Linux, where it also works with the X server to interface OpenGL with graphics hardware using "direct rendering interface" (DRI) drivers. You can see whether your particular DRI driver supports OpenGL 2.0 by running the &lt;tt&gt;glxinfo&lt;/tt&gt; command from an xterm. If OpenGL 2.0 isn't supported on your hardware, you can disable the driver to fall back to Mesa's software implementation. nVidia also provides their own proprietary OpenGL implementation for Linux targeting their own GPUs; this implementation should provide OpenGL 2.0 or later on any recent nVidia card.
&lt;/p&gt;
&lt;p&gt;
To install &lt;a href="http://www.opengl.org/resources/libraries/glut/"&gt;GLUT&lt;/a&gt; and &lt;a href="http://glew.sourceforge.net/"&gt;GLEW&lt;/a&gt;, look for the binary packages on their respective sites. MacOS X comes with GLUT preinstalled. Most Linux distributions have GLUT and GLEW available through their package system, though for GLUT, you may need to enable your distribution's optional "non-free" package repositories, since its license is not technically open source. There is an open-source GLUT clone called &lt;a href="http://openglut.sourceforge.net/"&gt;OpenGLUT&lt;/a&gt; if you're a stickler for such things.
&lt;/p&gt;
&lt;p&gt;
If you're a seasoned C programmer, you should be able to install these libraries and get them working in your development environment without any trouble. But before we get our hands dirty with any code, I'm going to go over some big-picture concepts. In this first chapter, I'm going to explain the graphics pipeline and the dataflow of a rendering job. In the next chapter, we'll write a simple "hello world" program that draws the contents of an image file to the screen, showing how the pipeline is put into practice.
&lt;/p&gt;

&lt;h3&gt;&lt;a name="gl1-pipeline"&gt;The graphics pipeline&lt;/a&gt;&lt;/h3&gt;
&lt;center&gt;&lt;img class="figure" src="http://duriansoftware.com/joe/media/gl1-pipeline-01.png"&gt;&lt;/center&gt;
&lt;p&gt;
Ever since the early days of real-time &lt;span class="smallcap"&gt;3d&lt;/span&gt;, the triangle has been the paintbrush with which scenes have been drawn. Although modern GPUs can perform all sorts of flashy effects to cover up this dirty secret, underneath all the shading, triangles are still the medium in which they work. The graphics pipeline that OpenGL implements reflects this: the host program fills OpenGL-managed memory buffers with arrays of vertices; these vertices are projected into screen space, assembled into triangles, and rasterized into pixel-sized fragments; finally, the fragments are assigned color values and drawn to the framebuffer. Modern GPUs get their flexibility by delegating the "project into screen space" and "assign color values" stages to uploadable programs called &lt;b&gt;shaders&lt;/b&gt;. Let's look at each stage in more detail:
&lt;/p&gt;

&lt;h3&gt;The vertex and element arrays&lt;/h3&gt;
&lt;p&gt;
A rendering job starts its journey through the pipeline in a set of one or more &lt;b&gt;vertex buffers&lt;/b&gt;, which are filled with arrays of &lt;b&gt;vertex attributes&lt;/b&gt;. These attributes are used as inputs to the vertex shader. Common vertex attributes include the location of the vertex in &lt;span class="smallcap"&gt;3d&lt;/span&gt; space, and one or more sets of texture coordinates that map the vertex to a sample point on one or more textures. The set of vertex buffers supplying data to a rendering job are collectively called the &lt;b&gt;vertex array&lt;/b&gt;. When a render job is submitted, we supply an additional &lt;b&gt;element array&lt;/b&gt;, an array of indexes into the vertex array that select which vertices get fed into the pipeline. The order of the indexes also controls how the vertices get assembled into triangles later on.
&lt;/p&gt;

&lt;h3&gt;Uniform state and textures&lt;/h3&gt;
&lt;p&gt;
A rendering job also has &lt;b&gt;uniform&lt;/b&gt; state, which provides a set of shared, read-only values to the shaders at each programmable stage of the pipeline. This allows the shader program to take parameters that don't change between vertices or fragments. The uniform state includes &lt;b&gt;textures&lt;/b&gt;, which are one-, two-, or three-dimensional arrays that can be sampled by shaders. As their name implies, textures are commonly used to map texture images onto surfaces. They can also be used as lookup tables for precalculated functions or as datasets for various kinds of effects.
&lt;/p&gt;

&lt;h3&gt;The vertex shader&lt;/h3&gt;
&lt;p&gt;
The GPU begins by reading each selected vertex out of the vertex array and running it through the &lt;b&gt;vertex shader&lt;/b&gt;, a program that takes a set of vertex attributes as inputs and outputs a new set of attributes, referred to as &lt;b&gt;varying&lt;/b&gt; values, that get fed to the rasterizer. At a minimum, the vertex shader calculates the projected &lt;b&gt;position&lt;/b&gt; of the vertex in screen space. The vertex shader can also generate other varying outputs, such as a color or texture coordinates, for the rasterizer to blend across the surface of the triangles connecting the vertex.
&lt;/p&gt;

&lt;h3&gt;Triangle assembly&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl1-triangle-assembly-01.png"&gt;
&lt;p&gt;The GPU then connects the projected vertices to form triangles. It does this by taking the vertices in the order specified by the element array and grouping them into sets of three. The vertices can be grouped in a few different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take every three elements as an independent triangle&lt;/li&gt;
&lt;li&gt;Make a &lt;b&gt;triangle strip&lt;/b&gt;, reusing the last two vertices of each triangle as the first two vertices of the next
&lt;li&gt;Make a &lt;b&gt;triangle fan&lt;/b&gt;, connecting the first element to every subsequent pair of elements
&lt;/ul&gt;
&lt;p&gt;The diagram shows how the three different modes behave. Strips and fans both require only one new index per triangle in the element array after the initial three, trading the flexibility of independent triangles for extra memory efficiency in the element array.&lt;/p&gt;

&lt;h3&gt;Rasterization&lt;/h3&gt;
&lt;img class="figure floated" src="http://duriansoftware.com/joe/media/gl1-rasterization-01.png"&gt;
&lt;p&gt;
The &lt;b&gt;rasterizer&lt;/b&gt; takes each triangle, clips it and discards parts that are outside of the screen, and breaks the remaining visible parts into pixel-sized &lt;b&gt;fragments&lt;/b&gt;. As mentioned above, the vertex shader's varying outputs are also interpolated across the rasterized surface of each triangle, assigning a smooth gradient of values to each fragment. For example, if the vertex shader assigns a color value to each vertex, the rasterizer will blend those colors across the pixelated surface as shown in the diagram.
&lt;/p&gt;

&lt;h3&gt;The fragment shader&lt;/h3&gt;
&lt;p&gt;
The generated fragments then pass through another program called the &lt;b&gt;fragment shader&lt;/b&gt;. The fragment shader receives the varying values output by the vertex shader and interpolated by the rasterizer as inputs. It outputs color and depth values that then get drawn into the framebuffer. Common fragment shader operations include texture mapping and lighting. Since the fragment shader runs independently for every pixel drawn, it can perform the most sophisticated special effects; however, it is also the most performance-sensitive part of the graphics pipeline.
&lt;/p&gt;

&lt;h3&gt;Framebuffers, testing, and blending&lt;/h3&gt;
&lt;p&gt;
A &lt;b&gt;framebuffer&lt;/b&gt; is the final destination for the rendering job's output. In addition to the default framebuffer OpenGL gives you to draw to the screen, most modern OpenGL implementations let you make &lt;b&gt;framebuffer objects&lt;/b&gt; that draw into offscreen &lt;b&gt;renderbuffers&lt;/b&gt; or into textures. Those textures can then be used as inputs to other rendering jobs. A framebuffer is more than a single &lt;span class="smallcap"&gt;2d&lt;/span&gt; image; in addition to one or more &lt;b&gt;color buffers&lt;/b&gt;, a framebuffer can have a &lt;b&gt;depth buffer&lt;/b&gt; and/or &lt;b&gt;stencil buffer&lt;/b&gt;, both of which optionally filter fragments before they are drawn to the framebuffer: &lt;b&gt;Depth testing&lt;/b&gt; discards fragments from objects that are behind the ones already drawn, and &lt;b&gt;stencil testing&lt;/b&gt; uses shapes drawn into the stencil buffer to constrain the drawable part of the framebuffer, "stencilling" the rendering job. Fragments that survive these two gauntlets have their color value &lt;b&gt;alpha blended&lt;/b&gt; with the color value they're overwriting, and the final color, depth, and stencil values are drawn into the corresponding buffers.
&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;
That's the process, from vertex buffers to framebuffer, that your data goes through when you make a single "draw" call in OpenGL. Rendering a scene usually involves multiple draw jobs, switching out textures, other uniform state, or shaders between passes and using the framebuffer's depth and stencil buffers to combine the results of each pass. Now that we've covered the general dataflow of &lt;span class="smallcap"&gt;3d&lt;/span&gt; rendering, we can write a simple program to see how OpenGL makes it all happen. Throughout the course of this tutorial, I'd love to &lt;script&gt;male_to('com', 'joe', 'duriansoftware', 'get your feedback')&lt;/script&gt;&amp;mdash;let me know if it's helping you or if anything doesn't make sense.
&lt;/p&gt;
&lt;h4&gt;&lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html"&gt;Table of Contents&lt;/a&gt; | &lt;a href="http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-2:-Hello-World:-The-Slideshow.html"&gt;Chapter 2 &amp;raquo;&lt;/a&gt;&lt;/h4&gt;

</description>
<dc:creator>Joe Groff</dc:creator>
<dc:date>2010-04-06T00:12:05+00:00</dc:date>
<guid>http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Chapter-1:-The-Graphics-Pipeline.html</guid>
</item>

</channel>
</rss>
